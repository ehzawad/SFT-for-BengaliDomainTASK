{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM7JXfjBPxGj"
      },
      "source": [
        "## 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eBmKI947PxGj"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers>=4.46.0 \"datasets>=3.0.0,<4.0.0\" trl>=0.12.0 peft>=0.13.0 accelerate>=1.0.0 bitsandbytes>=0.44.0 scikit-learn pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am0VQw1sPxGk"
      },
      "source": [
        "## 2. Check GPU & Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDUnWrxaPxGk",
        "outputId": "66001a80-7cad-4670-d714-ba7f0221b77b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: NVIDIA L4\n",
            "VRAM: 23.8 GB\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model output directory: /content/drive/MyDrive/models/smollm2-bengali-nid-intent\n",
            "\n",
            ">>> Upload your CSV files to: /content/drive/MyDrive\n",
            "    - sts_train.csv\n",
            "    - sts_eval.csv\n",
            "    - tag_answer.csv\n",
            "\n",
            "Or change DATASET_DIR to where your files are located.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check GPU\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU: {gpu_name}\")\n",
        "    print(f\"VRAM: {gpu_memory:.1f} GB\")\n",
        "else:\n",
        "    print(\"No GPU available! Go to Runtime > Change runtime type > GPU\")\n",
        "    raise RuntimeError(\"GPU required\")\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create output directory for model\n",
        "import os\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/models/smollm2-bengali-nid-intent\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"Model output directory: {OUTPUT_DIR}\")\n",
        "\n",
        "# ============================================================\n",
        "# DATASET PATH - Upload your CSVs to this folder in Google Drive\n",
        "# ============================================================\n",
        "DATASET_DIR = \"/content/drive/MyDrive\"\n",
        "os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"\\n>>> Upload your CSV files to: {DATASET_DIR}\")\n",
        "print(\"    - sts_train.csv\")\n",
        "print(\"    - sts_eval.csv\")\n",
        "print(\"    - tag_answer.csv\")\n",
        "print(\"\\nOr change DATASET_DIR to where your files are located.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6FWOIX7PxGk"
      },
      "source": [
        "## 3. Verify Dataset Files\n",
        "\n",
        "Make sure your CSV files are in Google Drive at the path shown above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Psn6M8lPxGk",
        "outputId": "76566e10-9cf4-418b-d270-c1637e23c082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset files status:\n",
            "  ✓ Found: sts_train.csv\n",
            "  ✓ Found: sts_eval.csv\n",
            "  ✓ Found: tag_answer.csv\n",
            "\n",
            "✓ All files found in /content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Check if files exist in Google Drive\n",
        "train_path = f\"{DATASET_DIR}/sts_train.csv\"\n",
        "eval_path = f\"{DATASET_DIR}/sts_eval.csv\"\n",
        "tag_path = f\"{DATASET_DIR}/tag_answer.csv\"\n",
        "\n",
        "files_status = {\n",
        "    \"sts_train.csv\": os.path.exists(train_path),\n",
        "    \"sts_eval.csv\": os.path.exists(eval_path),\n",
        "    \"tag_answer.csv\": os.path.exists(tag_path),\n",
        "}\n",
        "\n",
        "print(\"Dataset files status:\")\n",
        "for fname, exists in files_status.items():\n",
        "    status = \"✓ Found\" if exists else \"✗ Missing\"\n",
        "    print(f\"  {status}: {fname}\")\n",
        "\n",
        "if not all(files_status.values()):\n",
        "    missing = [f for f, exists in files_status.items() if not exists]\n",
        "    print(f\"\\n❌ Missing files: {missing}\")\n",
        "    print(f\"Please upload them to: {DATASET_DIR}\")\n",
        "    raise FileNotFoundError(f\"Missing dataset files in {DATASET_DIR}\")\n",
        "else:\n",
        "    print(f\"\\n✓ All files found in {DATASET_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Audd_GDVPxGk"
      },
      "source": [
        "## 4. Load and Analyze Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFzB23s0PxGk",
        "outputId": "8258044c-edfd-4c25-a649-1c4b3cf80459"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset files from Google Drive...\n",
            "Train samples: 78616\n",
            "Eval samples: 11457\n",
            "Unique tags in train: 407\n",
            "Unique tags in eval: 403\n",
            "Tags with answers: 407\n",
            "\n",
            "Sample from training data:\n",
            "  Question: \"একাউন্ট লক করা হয়েছে\" দেখাচ্ছে, সমাধান কী?\n",
            "  Tag: account_locked\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# Load CSV files from Google Drive\n",
        "print(\"Loading dataset files from Google Drive...\")\n",
        "train_df = pd.read_csv(train_path)\n",
        "eval_df = pd.read_csv(eval_path)\n",
        "tag_answer_df = pd.read_csv(tag_path)\n",
        "\n",
        "print(f\"Train samples: {len(train_df)}\")\n",
        "print(f\"Eval samples: {len(eval_df)}\")\n",
        "print(f\"Unique tags in train: {train_df['tag'].nunique()}\")\n",
        "print(f\"Unique tags in eval: {eval_df['tag'].nunique()}\")\n",
        "print(f\"Tags with answers: {len(tag_answer_df)}\")\n",
        "\n",
        "# Show sample\n",
        "print(f\"\\nSample from training data:\")\n",
        "print(f\"  Question: {train_df.iloc[0]['question']}\")\n",
        "print(f\"  Tag: {train_df.iloc[0]['tag']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUDNyNCnPxGk",
        "outputId": "1d8bde04-6782-44c9-ecb6-a1fbba789844"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique intents: 407\n",
            "\n",
            "Top 15 tags by frequency:\n",
            "  fraction: 494\n",
            "  permanent_address_change_fees: 381\n",
            "  spouse_name_correction_new: 231\n",
            "  parent_spouse_name_correct_or_add_document_new: 229\n",
            "  parents_name_correction_new: 226\n",
            "  goodbye: 218\n",
            "  picture_done_but_lost_or_no_sms_slip: 215\n",
            "  service_provided: 213\n",
            "  disability_no_hands_registration_procedure: 206\n",
            "  abroad_smart_card_collection_return: 206\n",
            "  reissue_urgent_card_delivery_time: 206\n",
            "  signature_to_fingerprint_reversal_not_allowed: 206\n",
            "  reissue_smart_card_download_not_available: 206\n",
            "  abroad_illegal_resident_nid: 206\n",
            "  abroad_embassy_walk_in_registration: 206\n"
          ]
        }
      ],
      "source": [
        "# Build intent labels from training data\n",
        "INTENT_TAGS = sorted(train_df['tag'].unique().tolist())\n",
        "print(f\"Total unique intents: {len(INTENT_TAGS)}\")\n",
        "\n",
        "# Create mappings\n",
        "ID2INTENT = {i: intent for i, intent in enumerate(INTENT_TAGS)}\n",
        "INTENT2ID = {intent: i for i, intent in enumerate(INTENT_TAGS)}\n",
        "\n",
        "# Show top 15 tags by frequency\n",
        "print(f\"\\nTop 15 tags by frequency:\")\n",
        "tag_counts = train_df['tag'].value_counts()\n",
        "for tag, count in tag_counts.head(15).items():\n",
        "    print(f\"  {tag}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu8ZekiyPxGk"
      },
      "source": [
        "## 5. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75zEHRXxPxGk",
        "outputId": "3f25da20-0211-4838-d9c4-65e8b3bfba0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: Qwen/Qwen2.5-0.5B-Instruct\n",
            "Number of intents: 407\n",
            "Max sequence length: 1024\n",
            "Few-shot examples: 3\n",
            "Context tags: 15\n",
            "Epochs: 8 (with early stopping patience=3)\n",
            "Batch size: 4 x 16 = 64 effective\n",
            "LoRA rank: 32\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CONFIGURATION - Few-Shot Task Learning with Qwen\n",
        "# ============================================================\n",
        "\n",
        "# Model - Qwen2.5-0.5B-Instruct for better multilingual support\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "\n",
        "# Data - Longer sequences for 5-shot examples + reasoning\n",
        "MAX_SEQ_LENGTH = 1024  # Optimized for L4/T4 VRAM (3-shot still fits)\n",
        "\n",
        "# Training (adjusted for longer sequences)\n",
        "NUM_EPOCHS = 8        # Keep at 2 with early stopping\n",
        "BATCH_SIZE = 4        # Memory-optimized\n",
        "EVAL_BATCH_SIZE = 8   # Memory-optimized\n",
        "GRAD_ACCUM_STEPS = 16 # Effective batch = 64 (2*32)\n",
        "LEARNING_RATE = 2e-5  # Lower for instruction tuning\n",
        "WARMUP_RATIO = 0.03   # Less warmup\n",
        "EARLY_STOPPING_PATIENCE = 3  # Stop if no improvement for 3 evals\n",
        "\n",
        "# LoRA config for Qwen\n",
        "LORA_R = 32           # Moderate rank\n",
        "LORA_ALPHA = 64       # 2x rank\n",
        "LORA_DROPOUT = 0.05\n",
        "LORA_TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "\n",
        "# Few-shot settings\n",
        "NUM_FEW_SHOT_EXAMPLES = 3  # Reduced for memory\n",
        "NUM_CONTEXT_TAGS = 15      # Reduced for memory\n",
        "\n",
        "# Seed\n",
        "SEED = 42\n",
        "\n",
        "# Instruction template (Bengali-aware)\n",
        "INSTRUCTION_TEMPLATE = \"Classify the intent of this Bengali customer query: {text}\"\n",
        "\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Number of intents: {len(INTENT_TAGS)}\")\n",
        "print(f\"Max sequence length: {MAX_SEQ_LENGTH}\")\n",
        "print(f\"Few-shot examples: {NUM_FEW_SHOT_EXAMPLES}\")\n",
        "print(f\"Context tags: {NUM_CONTEXT_TAGS}\")\n",
        "print(f\"Epochs: {NUM_EPOCHS} (with early stopping patience={EARLY_STOPPING_PATIENCE})\")\n",
        "print(f\"Batch size: {BATCH_SIZE} x {GRAD_ACCUM_STEPS} = {BATCH_SIZE * GRAD_ACCUM_STEPS} effective\")\n",
        "print(f\"LoRA rank: {LORA_R}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEB2hcySPxGk"
      },
      "source": [
        "## 6. Prepare Dataset for SFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmNvT_7PPxGk",
        "outputId": "b6944d0c-2424-491c-a9a1-f3e29b08c708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created descriptions for 407 tags\n",
            "Sample: ('abroad_address_change_impossible', 'উপজেলা নির্বাচন অফিসার বরাবর আবেদন করুন')\n",
            "Built example pool for 407 tags\n",
            "\n",
            "Formatting training data with 5-shot CoT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 78616/78616 [00:22<00:00, 3545.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formatting evaluation data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Eval: 100%|██████████| 11457/11457 [00:03<00:00, 3663.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train dataset: 78616 samples\n",
            "Eval dataset: 11457 samples\n",
            "\n",
            "============================================================\n",
            "TOKEN LENGTH DEBUG (sampling 100 examples)\n",
            "============================================================\n",
            "  Min word count: 430\n",
            "  Max word count: 563\n",
            "  Mean word count: 492.3\n",
            "  Max seq length setting: 1024\n",
            "\n",
            "============================================================\n",
            "SAMPLE FORMATTED TRAINING EXAMPLE:\n",
            "============================================================\n",
            "\n",
            "[SYSTEM]\n",
            "You are an intent classifier for Bengali NID (National ID) customer service.\n",
            "\n",
            "Your task: Given a Bengali customer query, analyze the text and classify it into the correct intent.\n",
            "\n",
            "Available intents with descriptions:\n",
            "- account_locked: ভুল তথ্য দিয়ে রেজিস্ট্রেশন বা লগইনের চেষ্টা করা হলে স্বয়ংক্রিয়ভাবে একাউন্টটি লক হয়ে যায়\n",
            "- account_locked_retrials: ভুল তথ্য দিয়ে রেজিস্ট্রেশন বা লগইনের জন্য তিনবারের অধিক চেষ্টা করা হলে স্বয়ংক্রিয়ভাবে একাউন্ট সাময়িক সময়ের জন্য লক করে দেওয়া হয়\n",
            "- account_locked_unloc...\n",
            "\n",
            "[USER]\n",
            "Classify the following Bengali query.\n",
            "\n",
            "Example 1:\n",
            "Query: আঙুলের ছাপ আপডেটের আবেদন কবে প্রসেসিং শেষ হবে?\n",
            "Reasoning: The query contains keywords related to \"fingerprint update applied not approved\". This matches the intent description: \"আপনার এই প্রশ্নের উত্তরটি জানতে ৯ বাটন চেপে কল সেন্টার প্রতিনিধির সঙ্গে সরাসরি কথা বলুন\". Based on the semantic match, this query belongs to fingerprint_update_applied_not_approved.\n",
            "Intent: fingerprint_update_applied_not_approved\n",
            "Example 2:\n",
            "Query: অনলাইন এর একাউন্ট...\n",
            "\n",
            "[ASSISTANT]\n",
            "Reasoning: The query contains keywords related to \"account locked\". This matches the intent description: \"ভুল তথ্য দিয়ে রেজিস্ট্রেশন বা লগইনের চেষ্টা করা হলে স্বয়ংক্রিয়ভাবে একাউন্টটি লক হয়ে যায়\". Based on the semantic match, this query belongs to account_locked.\n",
            "Intent: account_locked\n",
            "\n",
            "============================================================\n",
            "TAG DISTRIBUTION DEBUG\n",
            "============================================================\n",
            "  Top 10 tags in first 1000 samples:\n",
            "    account_locked: 187\n",
            "    account_locked_unlock_request: 181\n",
            "    account_locked_retrials: 175\n",
            "    address_change_card_download_process: 172\n",
            "    address_cahgne_but_no_post_office_or_office_code: 170\n",
            "    address_change_counselor_chairperson_signature: 115\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# ============================================================\n",
        "# STEP 1: Create Tag Descriptions from tag_answer.csv\n",
        "# ============================================================\n",
        "\n",
        "def create_tag_descriptions(tag_answer_df):\n",
        "    \"\"\"Extract Bengali descriptions from tag_answer.csv\"\"\"\n",
        "    descriptions = {}\n",
        "    for _, row in tag_answer_df.iterrows():\n",
        "        tag = row['tag']\n",
        "        answer = row['answer']\n",
        "        # First sentence (up to Bengali period ।) as description, max 150 chars\n",
        "        short_desc = answer.split('।')[0].strip()[:150]\n",
        "        descriptions[tag] = short_desc\n",
        "    return descriptions\n",
        "\n",
        "TAG_DESCRIPTIONS = create_tag_descriptions(tag_answer_df)\n",
        "print(f\"Created descriptions for {len(TAG_DESCRIPTIONS)} tags\")\n",
        "print(f\"Sample: {list(TAG_DESCRIPTIONS.items())[0]}\")\n",
        "\n",
        "# ============================================================\n",
        "# STEP 2: Build Few-Shot Example Pool\n",
        "# ============================================================\n",
        "\n",
        "def build_example_pool(train_df, examples_per_tag=10):\n",
        "    \"\"\"Create pool of examples for each tag\"\"\"\n",
        "    pool = {}\n",
        "    for tag in train_df['tag'].unique():\n",
        "        samples = train_df[train_df['tag'] == tag]['question'].tolist()\n",
        "        pool[tag] = samples[:min(examples_per_tag, len(samples))]\n",
        "    return pool\n",
        "\n",
        "EXAMPLE_POOL = build_example_pool(train_df, examples_per_tag=10)\n",
        "print(f\"Built example pool for {len(EXAMPLE_POOL)} tags\")\n",
        "\n",
        "# ============================================================\n",
        "# STEP 3: Reasoning Generation Function\n",
        "# ============================================================\n",
        "\n",
        "def generate_reasoning(question, tag, tag_description):\n",
        "    \"\"\"Generate detailed CoT reasoning for classification\"\"\"\n",
        "    tag_readable = tag.replace('_', ' ')\n",
        "    return (\n",
        "        f'The query contains keywords related to \"{tag_readable}\". '\n",
        "        f'This matches the intent description: \"{tag_description}\". '\n",
        "        f'Based on the semantic match, this query belongs to {tag}.'\n",
        "    )\n",
        "\n",
        "# ============================================================\n",
        "# STEP 4: Select Relevant Tags for Context\n",
        "# ============================================================\n",
        "\n",
        "def select_relevant_tags(target_tag, all_tags, k=20):\n",
        "    \"\"\"Select target tag + semantically similar distractors\"\"\"\n",
        "    selected = [target_tag]\n",
        "\n",
        "    # Same prefix tags (e.g., all address_change_* tags)\n",
        "    prefix = target_tag.split('_')[0]\n",
        "    same_prefix = [t for t in all_tags if t.startswith(prefix) and t != target_tag]\n",
        "    selected.extend(same_prefix[:7])\n",
        "\n",
        "    # Random diverse tags to fill remaining slots\n",
        "    remaining = [t for t in all_tags if t not in selected]\n",
        "    random.shuffle(remaining)\n",
        "    selected.extend(remaining[:k - len(selected)])\n",
        "\n",
        "    return selected[:k]\n",
        "\n",
        "# ============================================================\n",
        "# STEP 5: Main Formatting Function (Qwen Chat Template)\n",
        "# ============================================================\n",
        "\n",
        "def format_for_sft_few_shot(row, all_tags, tag_descriptions, example_pool, num_examples=5, num_tags=20):\n",
        "    \"\"\"Create Qwen chat format with few-shot examples and CoT reasoning\"\"\"\n",
        "    question = row['question']\n",
        "    target_tag = row['tag']\n",
        "\n",
        "    # Select relevant tags for context\n",
        "    relevant_tags = select_relevant_tags(target_tag, all_tags, k=num_tags)\n",
        "\n",
        "    # Format tag descriptions\n",
        "    tag_lines = []\n",
        "    for t in relevant_tags:\n",
        "        if t in tag_descriptions:\n",
        "            tag_lines.append(f\"- {t}: {tag_descriptions[t]}\")\n",
        "\n",
        "    # Select few-shot examples from different tags\n",
        "    available_example_tags = [t for t in relevant_tags if t in example_pool and t != target_tag]\n",
        "    if len(available_example_tags) < num_examples:\n",
        "        # Add more tags if not enough\n",
        "        extra_tags = [t for t in all_tags if t in example_pool and t not in available_example_tags]\n",
        "        random.shuffle(extra_tags)\n",
        "        available_example_tags.extend(extra_tags[:num_examples - len(available_example_tags)])\n",
        "\n",
        "    example_tags = random.sample(available_example_tags, min(num_examples, len(available_example_tags)))\n",
        "\n",
        "    # Format examples with reasoning\n",
        "    examples = []\n",
        "    for i, et in enumerate(example_tags, 1):\n",
        "        q = random.choice(example_pool[et])\n",
        "        r = generate_reasoning(q, et, tag_descriptions.get(et, et.replace('_', ' ')))\n",
        "        examples.append(f\"Example {i}:\\nQuery: {q}\\nReasoning: {r}\\nIntent: {et}\")\n",
        "\n",
        "    # Generate reasoning for target question\n",
        "    target_reasoning = generate_reasoning(\n",
        "        question, target_tag,\n",
        "        tag_descriptions.get(target_tag, target_tag.replace('_', ' '))\n",
        "    )\n",
        "\n",
        "    # Build Qwen chat format messages\n",
        "    system_content = f\"\"\"You are an intent classifier for Bengali NID (National ID) customer service.\n",
        "\n",
        "Your task: Given a Bengali customer query, analyze the text and classify it into the correct intent.\n",
        "\n",
        "Available intents with descriptions:\n",
        "{chr(10).join(tag_lines)}\n",
        "\n",
        "Instructions:\n",
        "1. Read the Bengali query carefully\n",
        "2. Identify key Bengali words/phrases\n",
        "3. Match them to the most relevant intent description\n",
        "4. Explain your reasoning\n",
        "5. Output the intent tag\"\"\"\n",
        "\n",
        "    user_content = f\"\"\"Classify the following Bengali query.\n",
        "\n",
        "{chr(10).join(examples)}\n",
        "\n",
        "Now classify this query:\n",
        "Query: {question}\"\"\"\n",
        "\n",
        "    assistant_content = f\"\"\"Reasoning: {target_reasoning}\n",
        "Intent: {target_tag}\"\"\"\n",
        "\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_content},\n",
        "            {\"role\": \"user\", \"content\": user_content},\n",
        "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
        "        ],\n",
        "        \"intent\": target_tag\n",
        "    }\n",
        "\n",
        "# ============================================================\n",
        "# STEP 6: Build Training and Eval Datasets\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nFormatting training data with 5-shot CoT...\")\n",
        "random.seed(SEED)  # For reproducibility\n",
        "train_formatted = []\n",
        "for _, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Train\"):\n",
        "    formatted = format_for_sft_few_shot(\n",
        "        row, INTENT_TAGS, TAG_DESCRIPTIONS, EXAMPLE_POOL,\n",
        "        num_examples=NUM_FEW_SHOT_EXAMPLES, num_tags=NUM_CONTEXT_TAGS\n",
        "    )\n",
        "    train_formatted.append(formatted)\n",
        "\n",
        "train_dataset = Dataset.from_list(train_formatted)\n",
        "\n",
        "print(\"Formatting evaluation data...\")\n",
        "eval_formatted = []\n",
        "for _, row in tqdm(eval_df.iterrows(), total=len(eval_df), desc=\"Eval\"):\n",
        "    formatted = format_for_sft_few_shot(\n",
        "        row, INTENT_TAGS, TAG_DESCRIPTIONS, EXAMPLE_POOL,\n",
        "        num_examples=NUM_FEW_SHOT_EXAMPLES, num_tags=NUM_CONTEXT_TAGS\n",
        "    )\n",
        "    eval_formatted.append(formatted)\n",
        "\n",
        "eval_dataset = Dataset.from_list(eval_formatted)\n",
        "\n",
        "print(f\"\\nTrain dataset: {len(train_dataset)} samples\")\n",
        "print(f\"Eval dataset: {len(eval_dataset)} samples\")\n",
        "\n",
        "# ============================================================\n",
        "# DEBUG: Token length statistics\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"TOKEN LENGTH DEBUG (sampling 100 examples)\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Sample 100 examples and check token lengths\n",
        "sample_indices = random.sample(range(len(train_formatted)), min(100, len(train_formatted)))\n",
        "token_lengths = []\n",
        "for idx in sample_indices:\n",
        "    sample = train_formatted[idx]\n",
        "    # Apply chat template to get full text\n",
        "    full_text = ''.join([m['content'] for m in sample['messages']])\n",
        "    tokens = len(full_text.split())  # Rough word count\n",
        "    token_lengths.append(tokens)\n",
        "\n",
        "print(f\"  Min word count: {min(token_lengths)}\")\n",
        "print(f\"  Max word count: {max(token_lengths)}\")\n",
        "print(f\"  Mean word count: {sum(token_lengths)/len(token_lengths):.1f}\")\n",
        "print(f\"  Max seq length setting: {MAX_SEQ_LENGTH}\")\n",
        "\n",
        "# Show formatted sample\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"SAMPLE FORMATTED TRAINING EXAMPLE:\")\n",
        "print(f\"{'='*60}\")\n",
        "sample = train_dataset[0]\n",
        "for msg in sample['messages']:\n",
        "    print(f\"\\n[{msg['role'].upper()}]\")\n",
        "    print(msg['content'][:500] + '...' if len(msg['content']) > 500 else msg['content'])\n",
        "\n",
        "# Debug: Show tag distribution in examples\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"TAG DISTRIBUTION DEBUG\")\n",
        "print(f\"{'='*60}\")\n",
        "tag_counts = {}\n",
        "for item in train_formatted[:1000]:  # Sample first 1000\n",
        "    tag = item['intent']\n",
        "    tag_counts[tag] = tag_counts.get(tag, 0) + 1\n",
        "top_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "print(f\"  Top 10 tags in first 1000 samples:\")\n",
        "for tag, count in top_tags:\n",
        "    print(f\"    {tag}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is59DGTAPxGk"
      },
      "source": [
        "## 7. Load Model and Apply LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yg871zcbPxGk",
        "outputId": "ff3e6f08-f46d-469d-afe4-f2f21a3b8656"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer: Qwen/Qwen2.5-0.5B-Instruct\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TOKENIZER DEBUG INFO\n",
            "============================================================\n",
            "  Vocab size: 151643\n",
            "  Pad token: <|endoftext|> (id=151643)\n",
            "  EOS token: <|im_end|> (id=151645)\n",
            "  Padding side: left\n",
            "  Has chat template: True\n",
            "  Chat template test: '<|im_start|>system\n",
            "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
            "<|...'\n",
            "\n",
            "============================================================\n",
            "LOADING MODEL: Qwen/Qwen2.5-0.5B-Instruct\n",
            "============================================================\n",
            "\n",
            "MODEL DEBUG INFO\n",
            "  Total parameters: 494,032,768\n",
            "  Trainable parameters: 494,032,768\n",
            "  Model dtype: torch.bfloat16\n",
            "  Device: cuda:0\n",
            "  Model class: Qwen2ForCausalLM\n",
            "  Hidden size: 896\n",
            "  Num layers: 24\n",
            "  Num heads: 14\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "\n",
        "# Load tokenizer\n",
        "print(f\"Loading tokenizer: {MODEL_NAME}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# IMPORTANT: Use left-padding for decoder-only models during batched generation\n",
        "tokenizer.padding_side = 'left'\n",
        "\n",
        "# Debug: Tokenizer info\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"TOKENIZER DEBUG INFO\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"  Vocab size: {tokenizer.vocab_size}\")\n",
        "print(f\"  Pad token: {tokenizer.pad_token} (id={tokenizer.pad_token_id})\")\n",
        "print(f\"  EOS token: {tokenizer.eos_token} (id={tokenizer.eos_token_id})\")\n",
        "print(f\"  Padding side: {tokenizer.padding_side}\")\n",
        "print(f\"  Has chat template: {tokenizer.chat_template is not None}\")\n",
        "\n",
        "# Test chat template\n",
        "test_msgs = [{\"role\": \"user\", \"content\": \"test\"}]\n",
        "test_prompt = tokenizer.apply_chat_template(test_msgs, tokenize=False, add_generation_prompt=True)\n",
        "print(f\"  Chat template test: '{test_prompt[:100]}...'\")\n",
        "\n",
        "# Load model\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"LOADING MODEL: {MODEL_NAME}\")\n",
        "print(f\"{'='*60}\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# Debug: Model info\n",
        "print(f\"\\nMODEL DEBUG INFO\")\n",
        "print(f\"  Total parameters: {model.num_parameters():,}\")\n",
        "print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "print(f\"  Model dtype: {model.dtype}\")\n",
        "print(f\"  Device: {model.device}\")\n",
        "print(f\"  Model class: {model.__class__.__name__}\")\n",
        "\n",
        "# Check model architecture\n",
        "if hasattr(model, 'config'):\n",
        "    print(f\"  Hidden size: {model.config.hidden_size}\")\n",
        "    print(f\"  Num layers: {model.config.num_hidden_layers}\")\n",
        "    print(f\"  Num heads: {model.config.num_attention_heads}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4Ea3EXDPxGk",
        "outputId": "a6f6b286-444b-4abf-955c-c293485d370e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "LORA CONFIGURATION DEBUG\n",
            "============================================================\n",
            "  Rank (r): 32\n",
            "  Alpha: 64\n",
            "  Dropout: 0.05\n",
            "  Target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
            "\n",
            "LoRA applied successfully!\n",
            "trainable params: 17,596,416 || all params: 511,629,184 || trainable%: 3.4393\n",
            "\n",
            "LoRA MODULES DEBUG:\n",
            "  Total LoRA modules: 1512\n",
            "  Sample LoRA layers: ['base_model.model.model.layers.0.self_attn.q_proj.lora_dropout', 'base_model.model.model.layers.0.self_attn.q_proj.lora_dropout.default', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B']\n"
          ]
        }
      ],
      "source": [
        "# Configure LoRA\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"LORA CONFIGURATION DEBUG\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"  Rank (r): {LORA_R}\")\n",
        "print(f\"  Alpha: {LORA_ALPHA}\")\n",
        "print(f\"  Dropout: {LORA_DROPOUT}\")\n",
        "print(f\"  Target modules: {LORA_TARGET_MODULES}\")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=LORA_TARGET_MODULES,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "print(f\"\\nLoRA applied successfully!\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Debug: Show which layers have LoRA\n",
        "print(f\"\\nLoRA MODULES DEBUG:\")\n",
        "lora_layers = [name for name, _ in model.named_modules() if 'lora' in name.lower()]\n",
        "print(f\"  Total LoRA modules: {len(lora_layers)}\")\n",
        "print(f\"  Sample LoRA layers: {lora_layers[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QkqsVJJPxGk"
      },
      "source": [
        "## 8. Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668,
          "referenced_widgets": [
            "0330756876c3461b9f0a5cd7af88c60a",
            "6dd2d50ec5fd4419aa7c6e13f0c3dff6",
            "134c76d886374391b0dc89036fcaf293",
            "9a37fdb681f14fbd8d0077f6d2a5c41e",
            "d7504239196842169e0ca519aa15c810",
            "b08cbde14fe047d6a100baa84f1c9b8a",
            "a4b99435b5ff404cabcef1181c07f5fa",
            "5eb7d76481dc43c980f7bcf235545588",
            "04405d9bec7a40b494e5ef1031312f64",
            "ff2c68b960f64320b3f56186ab960b93",
            "fa7b53015a594d5180a431f90d6dbe07",
            "d24ef17b494a44d8b09a0d002a8ce160",
            "eb31167477c3403f816ad56d52b270eb",
            "02ad53e59e474173b82af4cac8ef922b",
            "558f12a7b76a40938991573ff117cd9b",
            "ebbb1c5af8664afbac6a839b29132958",
            "5e2e5029ff974f9a84e3869a7b16c7fd",
            "56e02d0331e14c07a036eaba72b9540d",
            "04499d79018740f4998f5d0cc4bbfdbf",
            "113dbff4101b463e843bd4f5e8e17df1",
            "e65c234e787e4fc3a7e7cce942268ba1",
            "b3582c5fbf1644faa212e30b14ecec97",
            "0937db62a313452e8aa86284305f1e24",
            "14f3cef31151483cba15e135b9267425",
            "314d8c04b4c54c78b556fa10fb8b42ef",
            "be83a74e71e34326b2ceb31ed933075d",
            "b244e35611664ffca2c49282a7b3724a",
            "6517d470e0094ec0b7de0710f4152a7e",
            "889fc77dc2f644038b6c40c368708a12",
            "b32258734e6f4678b69f6f04747d2912",
            "f6089f4cc77c41539a7153656b90ac29",
            "52f49a3a055044e1a3e3f09a43a5da62",
            "6696374d4c374ef0aa80f5ad391b2922",
            "4992da1f3cae461bb2abea6032bffabd",
            "fbacc5739cb64ffb9f3f84a9dbffb954",
            "40347b1313c6493c96ca15e18e57533f",
            "76323bff8f0c493788863bea8d0d413f",
            "9682b4a873ee4594933eb8e86ab52132",
            "851e67ea3369442cac016805fdbccd7e",
            "da73aaa87a2848df94a0f05635d0d987",
            "f4ed8d92dff844ae9a10bfecce11edd2",
            "6917d0f94b684328aa3957c07aad52f6",
            "8e18c44f63344a328e23b7dd61d53814",
            "969be0adf83648729d5fc7632d2cdb47"
          ]
        },
        "id": "GE7tPCJJPxGl",
        "outputId": "641a2815-6545-462d-d40a-59b1c6ccf2a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CALLBACK INIT] Eval samples: 100, batch_size=8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/78616 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0330756876c3461b9f0a5cd7af88c60a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/78616 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d24ef17b494a44d8b09a0d002a8ce160"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing eval dataset:   0%|          | 0/11457 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0937db62a313452e8aa86284305f1e24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Truncating eval dataset:   0%|          | 0/11457 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4992da1f3cae461bb2abea6032bffabd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TRAINING SETUP DEBUG INFO\n",
            "============================================================\n",
            "  Model: Qwen/Qwen2.5-0.5B-Instruct\n",
            "  Max sequence length: 1024\n",
            "  Batch size: 4\n",
            "  Gradient accumulation: 16\n",
            "  Effective batch size: 64\n",
            "  Learning rate: 2e-05\n",
            "  Epochs: 8\n",
            "  Total training steps: 9824\n",
            "  Eval every: 500 steps\n",
            "  Save every: 1000 steps\n",
            "  Early stopping patience: 3\n",
            "\n",
            "MODEL STATE BEFORE TRAINING:\n",
            "  Model in training mode: True\n",
            "  Trainable params: 17,596,416\n",
            "  Total params: 511,629,184\n",
            "  Trainable %: 3.44%\n",
            "\n",
            "QUICK GENERATION TEST (before training):\n",
            "  Test output: 'Hello! How can I assist you today?'\n",
            "\n",
            "Trainer ready!\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "from transformers import set_seed, TrainerCallback, EarlyStoppingCallback\n",
        "from sklearn.metrics import accuracy_score\n",
        "import re\n",
        "\n",
        "# Set seed\n",
        "set_seed(SEED)\n",
        "\n",
        "# ============================================================\n",
        "# CUSTOM CALLBACK: Intent Accuracy + Early Stopping (Updated for CoT)\n",
        "# ============================================================\n",
        "\n",
        "def extract_intent_from_cot(response, intent_tags):\n",
        "    \"\"\"Extract intent from CoT response (looks for 'Intent: tag_name' pattern).\"\"\"\n",
        "    response = response.strip()\n",
        "\n",
        "    # Look for 'Intent: tag_name' pattern\n",
        "    intent_match = re.search(r'Intent:\\s*(\\S+)', response, re.IGNORECASE)\n",
        "    if intent_match:\n",
        "        predicted = intent_match.group(1).strip().lower()\n",
        "        # Find matching intent tag\n",
        "        for intent in intent_tags:\n",
        "            if intent.lower() == predicted:\n",
        "                return intent\n",
        "\n",
        "    # Fallback: check if any intent tag is in the response\n",
        "    response_lower = response.lower()\n",
        "    for intent in intent_tags:\n",
        "        if intent.lower() in response_lower:\n",
        "            return intent\n",
        "\n",
        "    return None\n",
        "\n",
        "def create_eval_prompt(question, tag_descriptions, example_pool, intent_tags, num_examples=3, num_tags=15):\n",
        "    \"\"\"Create evaluation prompt in the same format as training.\"\"\"\n",
        "    # Select relevant tags\n",
        "    relevant_tags = list(tag_descriptions.keys())[:num_tags]\n",
        "    tag_lines = [f\"- {t}: {tag_descriptions[t]}\" for t in relevant_tags if t in tag_descriptions]\n",
        "\n",
        "    # Select few examples\n",
        "    examples = []\n",
        "    available_tags = [t for t in relevant_tags if t in example_pool]\n",
        "    for i, et in enumerate(available_tags[:num_examples], 1):\n",
        "        q = example_pool[et][0] if example_pool[et] else \"sample query\"\n",
        "        desc = tag_descriptions.get(et, et.replace('_', ' '))\n",
        "        r = f'The query relates to \"{et.replace(\"_\", \" \")}\". This matches: \"{desc}\".'\n",
        "        examples.append(f\"Example {i}:\\nQuery: {q}\\nReasoning: {r}\\nIntent: {et}\")\n",
        "\n",
        "    system = f\"\"\"You are an intent classifier for Bengali NID customer service.\n",
        "\n",
        "Available intents:\n",
        "{chr(10).join(tag_lines[:num_tags])}\n",
        "\n",
        "Instructions: Analyze the query, explain reasoning, output intent tag.\"\"\"\n",
        "\n",
        "    user = f\"\"\"Classify this Bengali query.\n",
        "\n",
        "{chr(10).join(examples)}\n",
        "\n",
        "Now classify:\n",
        "Query: {question}\"\"\"\n",
        "\n",
        "    return [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}]\n",
        "\n",
        "class IntentAccuracyCallback(TrainerCallback):\n",
        "    \"\"\"Callback to compute intent accuracy during training with CoT format.\"\"\"\n",
        "\n",
        "    def __init__(self, eval_df, tokenizer, intent_tags, tag_descriptions, example_pool,\n",
        "                 max_seq_length, sample_size=300, batch_size=8, patience=3):\n",
        "        self.eval_sample = eval_df.sample(n=min(sample_size, len(eval_df)), random_state=42).reset_index(drop=True)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.intent_tags = intent_tags\n",
        "        self.tag_descriptions = tag_descriptions\n",
        "        self.example_pool = example_pool\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.batch_size = batch_size\n",
        "        self.patience = patience\n",
        "        self.best_accuracy = 0.0\n",
        "        self.no_improve_count = 0\n",
        "        self.history = []\n",
        "        self.train_losses = []\n",
        "        print(f\"[CALLBACK INIT] Eval samples: {len(self.eval_sample)}, batch_size={batch_size}\")\n",
        "\n",
        "    def on_train_begin(self, args, state, control, **kwargs):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"[DEBUG] TRAINING STARTED\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"  Max steps: {state.max_steps}\")\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs and 'loss' in logs:\n",
        "            self.train_losses.append(logs['loss'])\n",
        "            if state.global_step % 100 == 0:\n",
        "                avg = sum(self.train_losses[-10:]) / min(10, len(self.train_losses))\n",
        "                print(f\"  [Step {state.global_step}] loss={logs['loss']:.4f} | avg_10={avg:.4f}\")\n",
        "\n",
        "    def _compute_accuracy_batched(self, model):\n",
        "        \"\"\"Compute intent accuracy on sample using batched inference.\"\"\"\n",
        "        model.eval()\n",
        "        predictions = []\n",
        "        true_labels = self.eval_sample['tag'].tolist()\n",
        "        debug_responses = []\n",
        "\n",
        "        for i in range(0, len(self.eval_sample), self.batch_size):\n",
        "            batch_df = self.eval_sample.iloc[i:i+self.batch_size]\n",
        "\n",
        "            # Create prompts in chat format\n",
        "            batch_prompts = []\n",
        "            for q in batch_df['question']:\n",
        "                messages = create_eval_prompt(\n",
        "                    q, self.tag_descriptions, self.example_pool,\n",
        "                    self.intent_tags, num_examples=3, num_tags=15\n",
        "                )\n",
        "                prompt = self.tokenizer.apply_chat_template(\n",
        "                    messages, tokenize=False, add_generation_prompt=True\n",
        "                )\n",
        "                batch_prompts.append(prompt)\n",
        "\n",
        "            inputs = self.tokenizer(batch_prompts, return_tensors=\"pt\", padding=True,\n",
        "                                   truncation=True, max_length=self.max_seq_length)\n",
        "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=100,  # More tokens for reasoning + intent\n",
        "                    do_sample=False,\n",
        "                    pad_token_id=self.tokenizer.pad_token_id,\n",
        "                )\n",
        "\n",
        "            input_len = inputs['input_ids'].shape[1]\n",
        "            for idx, output in enumerate(outputs):\n",
        "                response = self.tokenizer.decode(output[input_len:], skip_special_tokens=True)\n",
        "                if i == 0 and idx < 3:\n",
        "                    debug_responses.append(response[:150])\n",
        "                predictions.append(extract_intent_from_cot(response, self.intent_tags))\n",
        "\n",
        "        # Debug output\n",
        "        print(f\"\\n    [DEBUG] Sample CoT outputs (first 3):\")\n",
        "        for resp in debug_responses[:3]:\n",
        "            print(f\"      Raw: '{resp}'\")\n",
        "        print(f\"    [DEBUG] Extracted intents vs true (first 3):\")\n",
        "        for pred, true in zip(predictions[:3], true_labels[:3]):\n",
        "            print(f\"      True: {true} | Predicted: {pred}\")\n",
        "\n",
        "        # Compute accuracy\n",
        "        valid_pairs = [(p, t) for p, t in zip(predictions, true_labels) if p is not None]\n",
        "        num_none = sum(1 for p in predictions if p is None)\n",
        "        print(f\"    [DEBUG] Valid predictions: {len(valid_pairs)}/{len(predictions)} ({num_none} returned None)\")\n",
        "\n",
        "        if len(valid_pairs) == 0:\n",
        "            return 0.0\n",
        "        valid_preds, valid_true = zip(*valid_pairs)\n",
        "        return accuracy_score(valid_true, valid_preds)\n",
        "\n",
        "    def on_evaluate(self, args, state, control, model, **kwargs):\n",
        "        accuracy = self._compute_accuracy_batched(model)\n",
        "        self.history.append({'step': state.global_step, 'intent_accuracy': accuracy})\n",
        "\n",
        "        print(f\"\\n>>> Intent Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "        if accuracy > self.best_accuracy + 0.001:\n",
        "            self.best_accuracy = accuracy\n",
        "            self.no_improve_count = 0\n",
        "            print(f\"    [NEW BEST] Best accuracy: {self.best_accuracy:.4f}\")\n",
        "        else:\n",
        "            self.no_improve_count += 1\n",
        "            print(f\"    No improvement for {self.no_improve_count}/{self.patience} evals\")\n",
        "\n",
        "        if self.no_improve_count >= self.patience:\n",
        "            print(f\"\\n*** EARLY STOPPING: Intent accuracy hasn't improved for {self.patience} evals ***\")\n",
        "            control.should_training_stop = True\n",
        "\n",
        "        return control\n",
        "\n",
        "# ============================================================\n",
        "# TRAINING CONFIGURATION\n",
        "# ============================================================\n",
        "\n",
        "training_args = SFTConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "\n",
        "    # Training schedule\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
        "\n",
        "    # Optimizer (lower LR for instruction tuning)\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"adamw_8bit\",\n",
        "\n",
        "    # Mixed precision\n",
        "    bf16=True,\n",
        "\n",
        "    # Logging & saving\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,  # Less frequent to save time\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=1000,  # Must be multiple of eval_steps\n",
        "    save_total_limit=3,\n",
        "\n",
        "    # Data - Chat template format\n",
        "    max_length=MAX_SEQ_LENGTH,\n",
        "    packing=False,\n",
        "\n",
        "    # Other\n",
        "    seed=SEED,\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        ")\n",
        "\n",
        "# Create callbacks with updated parameters\n",
        "intent_callback = IntentAccuracyCallback(\n",
        "    eval_df=eval_df,\n",
        "    tokenizer=tokenizer,\n",
        "    intent_tags=INTENT_TAGS,\n",
        "    tag_descriptions=TAG_DESCRIPTIONS,\n",
        "    example_pool=EXAMPLE_POOL,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    sample_size=100,  # Small sample to save memory\n",
        "    batch_size=EVAL_BATCH_SIZE,\n",
        "    patience=EARLY_STOPPING_PATIENCE,\n",
        ")\n",
        "\n",
        "# Early stopping on validation loss\n",
        "early_stopping_callback = EarlyStoppingCallback(\n",
        "    early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
        "    early_stopping_threshold=0.001,\n",
        ")\n",
        "\n",
        "# Initialize trainer with chat template support\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    args=training_args,\n",
        "    callbacks=[early_stopping_callback, intent_callback],\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# DEBUG: Training setup info\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"TRAINING SETUP DEBUG INFO\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Max sequence length: {MAX_SEQ_LENGTH}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Gradient accumulation: {GRAD_ACCUM_STEPS}\")\n",
        "print(f\"  Effective batch size: {BATCH_SIZE * GRAD_ACCUM_STEPS}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  Total training steps: {len(train_dataset) // (BATCH_SIZE * GRAD_ACCUM_STEPS) * NUM_EPOCHS}\")\n",
        "print(f\"  Eval every: {training_args.eval_steps} steps\")\n",
        "print(f\"  Save every: {training_args.save_steps} steps\")\n",
        "print(f\"  Early stopping patience: {EARLY_STOPPING_PATIENCE}\")\n",
        "\n",
        "# Debug: Check model state before training\n",
        "print(f\"\\nMODEL STATE BEFORE TRAINING:\")\n",
        "print(f\"  Model in training mode: {model.training}\")\n",
        "print(f\"  Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "print(f\"  Total params: {model.num_parameters():,}\")\n",
        "print(f\"  Trainable %: {100 * sum(p.numel() for p in model.parameters() if p.requires_grad) / model.num_parameters():.2f}%\")\n",
        "\n",
        "# Debug: Quick generation test\n",
        "print(f\"\\nQUICK GENERATION TEST (before training):\")\n",
        "test_prompt = tokenizer.apply_chat_template(\n",
        "    [{\"role\": \"user\", \"content\": \"Hello, test\"}],\n",
        "    tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "test_inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
        "with torch.no_grad():\n",
        "    test_out = model.generate(**test_inputs, max_new_tokens=20, do_sample=False, pad_token_id=tokenizer.pad_token_id)\n",
        "test_response = tokenizer.decode(test_out[0][test_inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "print(f\"  Test output: '{test_response[:100]}'\")\n",
        "print(f\"\\nTrainer ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "JofKkDKvPxGl",
        "outputId": "d3710bbb-a79f-4470-9bf0-ebb2ecc59c56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Dataset: ~78k Bengali NID queries, 407 intents\n",
            "Epochs: 8 (with early stopping)\n",
            "Early stopping patience: 3 evals\n",
            "\n",
            "During training you will see:\n",
            "  - Training loss (token generation)\n",
            "  - Validation loss (token generation)\n",
            "  - Intent Accuracy (tag detection on 500 samples)\n",
            "--------------------------------------------------\n",
            "\n",
            "============================================================\n",
            "[DEBUG] TRAINING STARTED\n",
            "============================================================\n",
            "  Max steps: 9832\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='41' max='9832' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  41/9832 10:27 < 43:44:53, 0.06 it/s, Epoch 0.03/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Train!\n",
        "print(\"Starting training...\")\n",
        "print(f\"Dataset: ~78k Bengali NID queries, 407 intents\")\n",
        "print(f\"Epochs: {NUM_EPOCHS} (with early stopping)\")\n",
        "print(f\"Early stopping patience: {EARLY_STOPPING_PATIENCE} evals\")\n",
        "print(\"\")\n",
        "print(\"During training you will see:\")\n",
        "print(\"  - Training loss (token generation)\")\n",
        "print(\"  - Validation loss (token generation)\")\n",
        "print(\"  - Intent Accuracy (tag detection on 500 samples)\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# ============================================================\n",
        "# POST-TRAINING DEBUG INFO\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"TRAINING COMPLETE - DEBUG SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"  Best intent accuracy: {intent_callback.best_accuracy:.4f} ({intent_callback.best_accuracy*100:.2f}%)\")\n",
        "print(f\"  Total evaluations: {len(intent_callback.history)}\")\n",
        "print(f\"  Training loss history (last 5): {intent_callback.train_losses[-5:] if intent_callback.train_losses else 'N/A'}\")\n",
        "\n",
        "# Show accuracy progression\n",
        "print(f\"\\nACCURACY PROGRESSION:\")\n",
        "for h in intent_callback.history:\n",
        "    print(f\"  Step {h['step']}: {h['intent_accuracy']:.4f}\")\n",
        "\n",
        "# Quick generation test after training\n",
        "print(f\"\\nPOST-TRAINING GENERATION TEST:\")\n",
        "test_q = \"একাউন্ট লক হয়ে গেছে কি করব?\"\n",
        "test_msgs = create_eval_prompt(test_q, TAG_DESCRIPTIONS, EXAMPLE_POOL, INTENT_TAGS)\n",
        "test_prompt = tokenizer.apply_chat_template(test_msgs, tokenize=False, add_generation_prompt=True)\n",
        "test_inputs = tokenizer(test_prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH).to(model.device)\n",
        "with torch.no_grad():\n",
        "    test_out = model.generate(**test_inputs, max_new_tokens=100, do_sample=False, pad_token_id=tokenizer.pad_token_id)\n",
        "test_response = tokenizer.decode(test_out[0][test_inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "print(f\"  Query: {test_q}\")\n",
        "print(f\"  Output: {test_response[:200]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx_nMO3lPxGl"
      },
      "source": [
        "## 9. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUJptZpfPxGl"
      },
      "outputs": [],
      "source": [
        "# Save model to Google Drive\n",
        "print(f\"Saving model to {OUTPUT_DIR}...\")\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "# Also save intent mappings\n",
        "import json\n",
        "with open(f\"{OUTPUT_DIR}/intent_mappings.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\"id2intent\": ID2INTENT, \"intent2id\": INTENT2ID}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Model saved successfully!\")\n",
        "print(f\"Files in {OUTPUT_DIR}:\")\n",
        "!ls -la {OUTPUT_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5nM-792PxGl"
      },
      "source": [
        "## 10. Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fw_Vo7pQPxGl"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def extract_intent_cot(response, intent_tags):\n",
        "    \"\"\"Extract intent from CoT model response.\"\"\"\n",
        "    response = response.strip()\n",
        "\n",
        "    # Look for 'Intent: tag_name' pattern\n",
        "    intent_match = re.search(r'Intent:\\s*(\\S+)', response, re.IGNORECASE)\n",
        "    if intent_match:\n",
        "        predicted = intent_match.group(1).strip().lower()\n",
        "        for intent in intent_tags:\n",
        "            if intent.lower() == predicted:\n",
        "                return intent\n",
        "\n",
        "    # Fallback: check if any intent tag appears\n",
        "    response_lower = response.lower()\n",
        "    for intent in intent_tags:\n",
        "        if intent.lower() in response_lower:\n",
        "            return intent\n",
        "\n",
        "    return None\n",
        "\n",
        "def create_inference_prompt(question, tag_descriptions, example_pool, num_examples=3, num_tags=15):\n",
        "    \"\"\"Create prompt for inference using few-shot format.\"\"\"\n",
        "    relevant_tags = list(tag_descriptions.keys())[:num_tags]\n",
        "    tag_lines = [f\"- {t}: {tag_descriptions[t]}\" for t in relevant_tags]\n",
        "\n",
        "    examples = []\n",
        "    available_tags = [t for t in relevant_tags if t in example_pool]\n",
        "    for i, et in enumerate(available_tags[:num_examples], 1):\n",
        "        q = example_pool[et][0] if example_pool[et] else \"sample\"\n",
        "        desc = tag_descriptions.get(et, '')\n",
        "        r = f'This query relates to \"{et.replace(\"_\", \" \")}\". Matches description: \"{desc[:80]}\"'\n",
        "        examples.append(f\"Example {i}:\\nQuery: {q}\\nReasoning: {r}\\nIntent: {et}\")\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": f\"\"\"You are an intent classifier for Bengali NID customer service.\n",
        "\n",
        "Available intents:\n",
        "{chr(10).join(tag_lines)}\n",
        "\n",
        "Instructions: Analyze query, explain reasoning, output intent tag.\"\"\"},\n",
        "        {\"role\": \"user\", \"content\": f\"\"\"{chr(10).join(examples)}\n",
        "\n",
        "Now classify:\n",
        "Query: {question}\"\"\"}\n",
        "    ]\n",
        "    return messages\n",
        "\n",
        "def evaluate_model_cot(model, tokenizer, eval_df, tag_descriptions, example_pool,\n",
        "                       batch_size=EVAL_BATCH_SIZE, num_samples=None):\n",
        "    \"\"\"Evaluate model with CoT few-shot format.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    if num_samples:\n",
        "        eval_df = eval_df.sample(n=min(num_samples, len(eval_df)), random_state=42).reset_index(drop=True)\n",
        "\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    raw_outputs = []\n",
        "\n",
        "    num_batches = (len(eval_df) + batch_size - 1) // batch_size\n",
        "    for i in tqdm(range(0, len(eval_df), batch_size), total=num_batches, desc=\"Evaluating (CoT)\"):\n",
        "        batch_df = eval_df.iloc[i:i+batch_size]\n",
        "\n",
        "        # Create prompts with chat template\n",
        "        batch_prompts = []\n",
        "        for q in batch_df['question']:\n",
        "            messages = create_inference_prompt(q, tag_descriptions, example_pool)\n",
        "            prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "            batch_prompts.append(prompt)\n",
        "\n",
        "        inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True,\n",
        "                          truncation=True, max_length=MAX_SEQ_LENGTH)\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=100,  # More tokens for reasoning\n",
        "                do_sample=False,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "            )\n",
        "\n",
        "        input_len = inputs['input_ids'].shape[1]\n",
        "        for j, output in enumerate(outputs):\n",
        "            response = tokenizer.decode(output[input_len:], skip_special_tokens=True)\n",
        "            if len(raw_outputs) < 5:\n",
        "                raw_outputs.append(response[:200])\n",
        "            predictions.append(extract_intent_cot(response, INTENT_TAGS))\n",
        "\n",
        "        true_labels.extend(batch_df['tag'].tolist())\n",
        "\n",
        "    # Show sample outputs\n",
        "    print(\"\\nSample CoT outputs:\")\n",
        "    for i, resp in enumerate(raw_outputs[:3]):\n",
        "        print(f\"  [{i+1}] {resp}\")\n",
        "\n",
        "    return predictions, true_labels\n",
        "\n",
        "# Evaluate on subset\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"EVALUATION DEBUG\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"  Model device: {model.device}\")\n",
        "print(f\"  Model training mode: {model.training}\")\n",
        "print(f\"  Batch size: {EVAL_BATCH_SIZE}\")\n",
        "print(f\"  Num samples: 1000\")\n",
        "print(f\"\\nStarting evaluation...\")\n",
        "\n",
        "predictions, true_labels = evaluate_model_cot(\n",
        "    model, tokenizer, eval_df, TAG_DESCRIPTIONS, EXAMPLE_POOL, num_samples=1000\n",
        ")\n",
        "\n",
        "# Debug: Prediction distribution\n",
        "print(f\"\\nPREDICTION DEBUG:\")\n",
        "null_preds = sum(1 for p in predictions if p is None)\n",
        "print(f\"  Null predictions: {null_preds}/{len(predictions)} ({100*null_preds/len(predictions):.1f}%)\")\n",
        "pred_counts = {}\n",
        "for p in predictions:\n",
        "    if p: pred_counts[p] = pred_counts.get(p, 0) + 1\n",
        "top_preds = sorted(pred_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "print(f\"  Top 5 predicted tags: {top_preds}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95mlnxeQPxGl"
      },
      "outputs": [],
      "source": [
        "# Compute metrics\n",
        "valid_mask = [p is not None for p in predictions]\n",
        "valid_preds = [INTENT2ID.get(p, -1) for p in predictions]\n",
        "valid_true = [INTENT2ID.get(t, -1) for t in true_labels]\n",
        "\n",
        "# Filter valid\n",
        "filtered_preds = [p for p, m in zip(valid_preds, valid_mask) if m and p != -1]\n",
        "filtered_true = [t for t, m, p in zip(valid_true, valid_mask, valid_preds) if m and p != -1]\n",
        "\n",
        "# Calculate metrics\n",
        "if len(filtered_preds) > 0:\n",
        "    accuracy = accuracy_score(filtered_true, filtered_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        filtered_true, filtered_preds, average=\"weighted\", zero_division=0\n",
        "    )\n",
        "else:\n",
        "    accuracy = precision = recall = f1 = 0.0\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Total samples: {len(predictions)}\")\n",
        "print(f\"Valid predictions: {sum(valid_mask)} ({100*sum(valid_mask)/len(predictions):.1f}%)\")\n",
        "print(f\"\")\n",
        "print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Show top confusions\n",
        "print(\"\\nTop 10 Confusions:\")\n",
        "confusions = [(t, p) for p, t in zip(predictions, true_labels) if p != t and p is not None]\n",
        "for (true, pred), count in Counter(confusions).most_common(10):\n",
        "    print(f\"  {true} -> {pred}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo2IuxB3PxGl"
      },
      "source": [
        "## 11. Interactive Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygtSMSt4PxGl"
      },
      "outputs": [],
      "source": [
        "def classify_intent_cot(query, model, tokenizer, tag_descriptions, example_pool):\n",
        "    \"\"\"Classify intent for a single Bengali query using CoT format.\"\"\"\n",
        "    messages = create_inference_prompt(query, tag_descriptions, example_pool)\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "    intent = extract_intent_cot(response, INTENT_TAGS)\n",
        "\n",
        "    return intent, response\n",
        "\n",
        "# Get answer for intent from tag_answer_df\n",
        "def get_answer(intent):\n",
        "    \"\"\"Get Bengali answer for an intent.\"\"\"\n",
        "    row = tag_answer_df[tag_answer_df['tag'] == intent]\n",
        "    if len(row) > 0:\n",
        "        return row.iloc[0]['answer']\n",
        "    return \"উত্তর পাওয়া যায়নি।\"\n",
        "\n",
        "# Test with sample Bengali queries\n",
        "test_queries = [\n",
        "    \"আমার এনআইডি একাউন্ট লক হয়ে গেছে, কিভাবে আনলক করবো?\",\n",
        "    \"কার্ড হারিয়ে গেলে কি করতে হবে?\",\n",
        "    \"জাতীয় পরিচয়পত্রে নাম সংশোধন করতে চাই\",\n",
        "    \"ভোটার আইডি কার্ডের ঠিকানা পরিবর্তন করতে কি কি লাগবে?\",\n",
        "    \"স্মার্ট কার্ড কবে পাবো?\",\n",
        "]\n",
        "\n",
        "# Test with CoT inference\n",
        "print(\"Testing with Bengali queries (CoT inference):\")\n",
        "print(\"=\" * 70)\n",
        "for query in test_queries:\n",
        "    intent, reasoning = classify_intent_cot(query, model, tokenizer, TAG_DESCRIPTIONS, EXAMPLE_POOL)\n",
        "    answer = get_answer(intent) if intent else \"Intent not recognized\"\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Model output: {reasoning[:150]}...\" if len(reasoning) > 150 else f\"Model output: {reasoning}\")\n",
        "    print(f\"Intent: {intent}\")\n",
        "    print(f\"Answer: {answer[:100]}...\" if len(answer) > 100 else f\"Answer: {answer}\")\n",
        "    print(\"-\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPQ1tXTnPxGl"
      },
      "source": [
        "## 12. Push to HuggingFace Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azBWZepHPxGl"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFcKckLiPxGl"
      },
      "outputs": [],
      "source": [
        "# Push model to Hub\n",
        "HF_REPO_NAME = \"ehzawad/smollm2-bengali-nid-intent\"\n",
        "\n",
        "print(f\"Pushing model to HuggingFace Hub: {HF_REPO_NAME}\")\n",
        "\n",
        "model.push_to_hub(HF_REPO_NAME)\n",
        "tokenizer.push_to_hub(HF_REPO_NAME)\n",
        "\n",
        "print(f\"\\nModel uploaded successfully!\")\n",
        "print(f\"View at: https://huggingface.co/{HF_REPO_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1YJ41MtPxGl"
      },
      "source": [
        "## Done!\n",
        "\n",
        "Your Bengali NID intent classification model is now:\n",
        "- Saved to Google Drive at: `/content/drive/MyDrive/models/smollm2-bengali-nid-intent`\n",
        "- Pushed to HuggingFace Hub\n",
        "\n",
        "To load the model later:\n",
        "```python\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
        "model = PeftModel.from_pretrained(base_model, \"ehzawad/smollm2-bengali-nid-intent\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ehzawad/smollm2-bengali-nid-intent\")\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0330756876c3461b9f0a5cd7af88c60a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6dd2d50ec5fd4419aa7c6e13f0c3dff6",
              "IPY_MODEL_134c76d886374391b0dc89036fcaf293",
              "IPY_MODEL_9a37fdb681f14fbd8d0077f6d2a5c41e"
            ],
            "layout": "IPY_MODEL_d7504239196842169e0ca519aa15c810"
          }
        },
        "6dd2d50ec5fd4419aa7c6e13f0c3dff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b08cbde14fe047d6a100baa84f1c9b8a",
            "placeholder": "​",
            "style": "IPY_MODEL_a4b99435b5ff404cabcef1181c07f5fa",
            "value": "Tokenizing train dataset: 100%"
          }
        },
        "134c76d886374391b0dc89036fcaf293": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5eb7d76481dc43c980f7bcf235545588",
            "max": 78616,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_04405d9bec7a40b494e5ef1031312f64",
            "value": 78616
          }
        },
        "9a37fdb681f14fbd8d0077f6d2a5c41e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff2c68b960f64320b3f56186ab960b93",
            "placeholder": "​",
            "style": "IPY_MODEL_fa7b53015a594d5180a431f90d6dbe07",
            "value": " 78616/78616 [05:46&lt;00:00, 270.84 examples/s]"
          }
        },
        "d7504239196842169e0ca519aa15c810": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b08cbde14fe047d6a100baa84f1c9b8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4b99435b5ff404cabcef1181c07f5fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5eb7d76481dc43c980f7bcf235545588": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04405d9bec7a40b494e5ef1031312f64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff2c68b960f64320b3f56186ab960b93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa7b53015a594d5180a431f90d6dbe07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d24ef17b494a44d8b09a0d002a8ce160": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb31167477c3403f816ad56d52b270eb",
              "IPY_MODEL_02ad53e59e474173b82af4cac8ef922b",
              "IPY_MODEL_558f12a7b76a40938991573ff117cd9b"
            ],
            "layout": "IPY_MODEL_ebbb1c5af8664afbac6a839b29132958"
          }
        },
        "eb31167477c3403f816ad56d52b270eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e2e5029ff974f9a84e3869a7b16c7fd",
            "placeholder": "​",
            "style": "IPY_MODEL_56e02d0331e14c07a036eaba72b9540d",
            "value": "Truncating train dataset: 100%"
          }
        },
        "02ad53e59e474173b82af4cac8ef922b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04499d79018740f4998f5d0cc4bbfdbf",
            "max": 78616,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_113dbff4101b463e843bd4f5e8e17df1",
            "value": 78616
          }
        },
        "558f12a7b76a40938991573ff117cd9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e65c234e787e4fc3a7e7cce942268ba1",
            "placeholder": "​",
            "style": "IPY_MODEL_b3582c5fbf1644faa212e30b14ecec97",
            "value": " 78616/78616 [00:01&lt;00:00, 60954.49 examples/s]"
          }
        },
        "ebbb1c5af8664afbac6a839b29132958": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e2e5029ff974f9a84e3869a7b16c7fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56e02d0331e14c07a036eaba72b9540d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04499d79018740f4998f5d0cc4bbfdbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "113dbff4101b463e843bd4f5e8e17df1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e65c234e787e4fc3a7e7cce942268ba1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3582c5fbf1644faa212e30b14ecec97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0937db62a313452e8aa86284305f1e24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14f3cef31151483cba15e135b9267425",
              "IPY_MODEL_314d8c04b4c54c78b556fa10fb8b42ef",
              "IPY_MODEL_be83a74e71e34326b2ceb31ed933075d"
            ],
            "layout": "IPY_MODEL_b244e35611664ffca2c49282a7b3724a"
          }
        },
        "14f3cef31151483cba15e135b9267425": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6517d470e0094ec0b7de0710f4152a7e",
            "placeholder": "​",
            "style": "IPY_MODEL_889fc77dc2f644038b6c40c368708a12",
            "value": "Tokenizing eval dataset: 100%"
          }
        },
        "314d8c04b4c54c78b556fa10fb8b42ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b32258734e6f4678b69f6f04747d2912",
            "max": 11457,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6089f4cc77c41539a7153656b90ac29",
            "value": 11457
          }
        },
        "be83a74e71e34326b2ceb31ed933075d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52f49a3a055044e1a3e3f09a43a5da62",
            "placeholder": "​",
            "style": "IPY_MODEL_6696374d4c374ef0aa80f5ad391b2922",
            "value": " 11457/11457 [00:51&lt;00:00, 248.75 examples/s]"
          }
        },
        "b244e35611664ffca2c49282a7b3724a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6517d470e0094ec0b7de0710f4152a7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "889fc77dc2f644038b6c40c368708a12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b32258734e6f4678b69f6f04747d2912": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6089f4cc77c41539a7153656b90ac29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "52f49a3a055044e1a3e3f09a43a5da62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6696374d4c374ef0aa80f5ad391b2922": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4992da1f3cae461bb2abea6032bffabd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fbacc5739cb64ffb9f3f84a9dbffb954",
              "IPY_MODEL_40347b1313c6493c96ca15e18e57533f",
              "IPY_MODEL_76323bff8f0c493788863bea8d0d413f"
            ],
            "layout": "IPY_MODEL_9682b4a873ee4594933eb8e86ab52132"
          }
        },
        "fbacc5739cb64ffb9f3f84a9dbffb954": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_851e67ea3369442cac016805fdbccd7e",
            "placeholder": "​",
            "style": "IPY_MODEL_da73aaa87a2848df94a0f05635d0d987",
            "value": "Truncating eval dataset: 100%"
          }
        },
        "40347b1313c6493c96ca15e18e57533f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4ed8d92dff844ae9a10bfecce11edd2",
            "max": 11457,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6917d0f94b684328aa3957c07aad52f6",
            "value": 11457
          }
        },
        "76323bff8f0c493788863bea8d0d413f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e18c44f63344a328e23b7dd61d53814",
            "placeholder": "​",
            "style": "IPY_MODEL_969be0adf83648729d5fc7632d2cdb47",
            "value": " 11457/11457 [00:00&lt;00:00, 42299.50 examples/s]"
          }
        },
        "9682b4a873ee4594933eb8e86ab52132": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "851e67ea3369442cac016805fdbccd7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da73aaa87a2848df94a0f05635d0d987": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4ed8d92dff844ae9a10bfecce11edd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6917d0f94b684328aa3957c07aad52f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e18c44f63344a328e23b7dd61d53814": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "969be0adf83648729d5fc7632d2cdb47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}