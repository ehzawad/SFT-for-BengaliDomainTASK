{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM7JXfjBPxGj"
      },
      "source": [
        "## 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eBmKI947PxGj"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers>=4.46.0 \"datasets>=3.0.0,<4.0.0\" trl>=0.12.0 peft>=0.13.0 accelerate>=1.0.0 bitsandbytes>=0.44.0 scikit-learn pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 HuggingFace Token (optional, set once)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# HuggingFace credentials (set once at the top)\n",
        "HF_TOKEN = \"\"  # Paste your token here\n",
        "HF_USERNAME = \"ehzawad\"  # Change if needed\n",
        "HF_REPO_SUFFIX = \"bn-nid-intent-qwen2.5-0.5b\"  # Contextual repo name\n",
        "HF_RUN_TAG = \"\"  # Optional: set a custom tag (auto-filled if empty)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am0VQw1sPxGk"
      },
      "source": [
        "## 2. Check GPU & Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDUnWrxaPxGk",
        "outputId": "66001a80-7cad-4670-d714-ba7f0221b77b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: NVIDIA L4\n",
            "VRAM: 23.8 GB\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model output directory: /content/drive/MyDrive/models/smollm2-bengali-nid-intent\n",
            "\n",
            ">>> Upload your CSV files to: /content/drive/MyDrive\n",
            "    - sts_train.csv\n",
            "    - sts_eval.csv\n",
            "    - tag_answer.csv\n",
            "\n",
            "Or change DATASET_DIR to where your files are located.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check GPU\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU: {gpu_name}\")\n",
        "    print(f\"VRAM: {gpu_memory:.1f} GB\")\n",
        "else:\n",
        "    print(\"No GPU available! Go to Runtime > Change runtime type > GPU\")\n",
        "    raise RuntimeError(\"GPU required\")\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create output directories for model/checkpoints\n",
        "import os\n",
        "\n",
        "RUN_NAME = \"smollm2-bengali-nid-intent\"  # Stable name for reloads\n",
        "OUTPUT_DIR = f\"/content/drive/MyDrive/models/{RUN_NAME}\"\n",
        "DPO_OUTPUT_DIR = f\"{OUTPUT_DIR}-dpo\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(DPO_OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"Model output directory: {OUTPUT_DIR}\")\n",
        "print(f\"DPO output directory: {DPO_OUTPUT_DIR}\")\n",
        "\n",
        "# ============================================================\n",
        "# DATASET PATH - Upload your CSVs to this folder in Google Drive\n",
        "# ============================================================\n",
        "DATASET_DIR = \"/content/drive/MyDrive\"\n",
        "os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"\\n>>> Upload your CSV files to: {DATASET_DIR}\")\n",
        "print(\"    - sts_train.csv\")\n",
        "print(\"    - sts_eval.csv\")\n",
        "print(\"    - tag_answer.csv\")\n",
        "print(\"\\nOr change DATASET_DIR to where your files are located.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6FWOIX7PxGk"
      },
      "source": [
        "## 3. Verify Dataset Files\n",
        "\n",
        "Make sure your CSV files are in Google Drive at the path shown above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Psn6M8lPxGk",
        "outputId": "76566e10-9cf4-418b-d270-c1637e23c082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset files status:\n",
            "  ✓ Found: sts_train.csv\n",
            "  ✓ Found: sts_eval.csv\n",
            "  ✓ Found: tag_answer.csv\n",
            "\n",
            "✓ All files found in /content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Check if files exist in Google Drive\n",
        "train_path = f\"{DATASET_DIR}/sts_train.csv\"\n",
        "eval_path = f\"{DATASET_DIR}/sts_eval.csv\"\n",
        "tag_path = f\"{DATASET_DIR}/tag_answer.csv\"\n",
        "\n",
        "files_status = {\n",
        "    \"sts_train.csv\": os.path.exists(train_path),\n",
        "    \"sts_eval.csv\": os.path.exists(eval_path),\n",
        "    \"tag_answer.csv\": os.path.exists(tag_path),\n",
        "}\n",
        "\n",
        "print(\"Dataset files status:\")\n",
        "for fname, exists in files_status.items():\n",
        "    status = \"✓ Found\" if exists else \"✗ Missing\"\n",
        "    print(f\"  {status}: {fname}\")\n",
        "\n",
        "if not all(files_status.values()):\n",
        "    missing = [f for f, exists in files_status.items() if not exists]\n",
        "    print(f\"\\n❌ Missing files: {missing}\")\n",
        "    print(f\"Please upload them to: {DATASET_DIR}\")\n",
        "    raise FileNotFoundError(f\"Missing dataset files in {DATASET_DIR}\")\n",
        "else:\n",
        "    print(f\"\\n✓ All files found in {DATASET_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Audd_GDVPxGk"
      },
      "source": [
        "## 4. Load and Analyze Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFzB23s0PxGk",
        "outputId": "8258044c-edfd-4c25-a649-1c4b3cf80459"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset files from Google Drive...\n",
            "Train samples: 78616\n",
            "Eval samples: 11457\n",
            "Unique tags in train: 407\n",
            "Unique tags in eval: 403\n",
            "Tags with answers: 407\n",
            "\n",
            "Sample from training data:\n",
            "  Question: \"একাউন্ট লক করা হয়েছে\" দেখাচ্ছে, সমাধান কী?\n",
            "  Tag: account_locked\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# Load CSV files from Google Drive\n",
        "print(\"Loading dataset files from Google Drive...\")\n",
        "train_df = pd.read_csv(train_path)\n",
        "# ============================================================\n",
        "# SAMPLE 50% OF TRAINING DATA FOR FASTER TRAINING\n",
        "# ============================================================\n",
        "train_df = train_df.sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
        "print(\">>> Using 50% of training data for faster training <<<\")\n",
        "eval_df = pd.read_csv(eval_path)\n",
        "tag_answer_df = pd.read_csv(tag_path)\n",
        "\n",
        "print(f\"Train samples: {len(train_df)}\")\n",
        "print(f\"Eval samples: {len(eval_df)}\")\n",
        "print(f\"Unique tags in train: {train_df['tag'].nunique()}\")\n",
        "print(f\"Unique tags in eval: {eval_df['tag'].nunique()}\")\n",
        "print(f\"Tags with answers: {len(tag_answer_df)}\")\n",
        "\n",
        "# Show sample\n",
        "print(f\"\\nSample from training data:\")\n",
        "print(f\"  Question: {train_df.iloc[0]['question']}\")\n",
        "print(f\"  Tag: {train_df.iloc[0]['tag']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUDNyNCnPxGk",
        "outputId": "1d8bde04-6782-44c9-ecb6-a1fbba789844"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique intents: 407\n",
            "\n",
            "Top 15 tags by frequency:\n",
            "  fraction: 494\n",
            "  permanent_address_change_fees: 381\n",
            "  spouse_name_correction_new: 231\n",
            "  parent_spouse_name_correct_or_add_document_new: 229\n",
            "  parents_name_correction_new: 226\n",
            "  goodbye: 218\n",
            "  picture_done_but_lost_or_no_sms_slip: 215\n",
            "  service_provided: 213\n",
            "  disability_no_hands_registration_procedure: 206\n",
            "  abroad_smart_card_collection_return: 206\n",
            "  reissue_urgent_card_delivery_time: 206\n",
            "  signature_to_fingerprint_reversal_not_allowed: 206\n",
            "  reissue_smart_card_download_not_available: 206\n",
            "  abroad_illegal_resident_nid: 206\n",
            "  abroad_embassy_walk_in_registration: 206\n"
          ]
        }
      ],
      "source": [
        "# Build intent labels from training data\n",
        "INTENT_TAGS = sorted(train_df['tag'].unique().tolist())\n",
        "print(f\"Total unique intents: {len(INTENT_TAGS)}\")\n",
        "\n",
        "# Create mappings\n",
        "ID2INTENT = {i: intent for i, intent in enumerate(INTENT_TAGS)}\n",
        "INTENT2ID = {intent: i for i, intent in enumerate(INTENT_TAGS)}\n",
        "\n",
        "# Show top 15 tags by frequency\n",
        "print(f\"\\nTop 15 tags by frequency:\")\n",
        "tag_counts = train_df['tag'].value_counts()\n",
        "for tag, count in tag_counts.head(15).items():\n",
        "    print(f\"  {tag}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu8ZekiyPxGk"
      },
      "source": [
        "## 5. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75zEHRXxPxGk",
        "outputId": "3f25da20-0211-4838-d9c4-65e8b3bfba0c"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CONFIGURATION - Direct Classification (No CoT, No Few-Shot)\n",
        "# ============================================================\n",
        "\n",
        "# Model - Qwen2.5-0.5B-Instruct for multilingual support\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "\n",
        "# Data - Short sequences since output is just a tag\n",
        "MAX_SEQ_LENGTH = 256  # Minimal: query + tag only\n",
        "\n",
        "# Training - Optimized for L4 24GB (~18GB usage)\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 16       # Large batch for short sequences\n",
        "EVAL_BATCH_SIZE = 32  # Even larger for eval (no gradients)\n",
        "GRAD_ACCUM_STEPS = 4  # Effective batch = 64\n",
        "LEARNING_RATE = 2e-4  # Higher LR for direct classification\n",
        "WARMUP_RATIO = 0.05\n",
        "EARLY_STOPPING_PATIENCE = 3\n",
        "\n",
        "# LoRA config\n",
        "LORA_R = 32\n",
        "LORA_ALPHA = 64\n",
        "LORA_DROPOUT = 0.05\n",
        "LORA_TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "\n",
        "# Seed\n",
        "SEED = 42\n",
        "\n",
        "# Checkpoint resume (optional)\n",
        "RESUME_FROM_CHECKPOINT = None  # e.g., f\"{OUTPUT_DIR}/checkpoint-1500\"\n",
        "DPO_RESUME_FROM_CHECKPOINT = None  # e.g., f\"{DPO_OUTPUT_DIR}/checkpoint-100\"\n",
        "\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Number of intents: {len(INTENT_TAGS)}\")\n",
        "print(f\"Max sequence length: {MAX_SEQ_LENGTH}\")\n",
        "print(f\"Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Batch size: {BATCH_SIZE} x {GRAD_ACCUM_STEPS} = {BATCH_SIZE * GRAD_ACCUM_STEPS} effective\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"LoRA rank: {LORA_R}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEB2hcySPxGk"
      },
      "source": [
        "## 6. Prepare Dataset for SFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmNvT_7PPxGk",
        "outputId": "b6944d0c-2424-491c-a9a1-f3e29b08c708"
      },
      "outputs": [],
      "source": "from datasets import Dataset\nfrom tqdm import tqdm\nimport random\n\n# ============================================================\n# SIMPLIFIED FORMAT: Direct Classification\n# System: You are an intent classifier. Output only the intent tag.\n# User: {query}\n# Assistant: {tag}\n# ============================================================\n\n# System prompt - minimal and direct\nSYSTEM_PROMPT = \"You are an intent classifier for Bengali NID customer service. Output only the intent tag, nothing else.\"\n\ndef format_for_direct_classification(row):\n    \"\"\"Create minimal chat format for direct intent classification.\"\"\"\n    question = row['question']\n    target_tag = row['tag']\n    \n    return {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n            {\"role\": \"user\", \"content\": question},\n            {\"role\": \"assistant\", \"content\": target_tag}\n        ],\n        \"intent\": target_tag\n    }\n\n# ============================================================\n# Build Training and Eval Datasets\n# ============================================================\n\nprint(\"Formatting training data (direct classification)...\")\nrandom.seed(SEED)\ntrain_formatted = []\nfor _, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Train\"):\n    formatted = format_for_direct_classification(row)\n    train_formatted.append(formatted)\n\ntrain_dataset = Dataset.from_list(train_formatted)\n\nprint(\"Formatting evaluation data...\")\neval_formatted = []\nfor _, row in tqdm(eval_df.iterrows(), total=len(eval_df), desc=\"Eval\"):\n    formatted = format_for_direct_classification(row)\n    eval_formatted.append(formatted)\n\neval_dataset = Dataset.from_list(eval_formatted)\n\nprint(f\"\\nTrain dataset: {len(train_dataset)} samples\")\nprint(f\"Eval dataset: {len(eval_dataset)} samples\")\n\n# ============================================================\n# DEBUG: Token length statistics\n# ============================================================\nprint(f\"\\n{'='*60}\")\nprint(\"TOKEN LENGTH DEBUG (sampling 100 examples)\")\nprint(f\"{'='*60}\")\n\nsample_indices = random.sample(range(len(train_formatted)), min(100, len(train_formatted)))\ntoken_lengths = []\nfor idx in sample_indices:\n    sample = train_formatted[idx]\n    # Concatenate all message contents\n    full_text = ''.join([m['content'] for m in sample['messages']])\n    tokens = len(full_text.split())  # Rough word count\n    token_lengths.append(tokens)\n\nprint(f\"  Min word count: {min(token_lengths)}\")\nprint(f\"  Max word count: {max(token_lengths)}\")\nprint(f\"  Mean word count: {sum(token_lengths)/len(token_lengths):.1f}\")\nprint(f\"  Max seq length setting: {MAX_SEQ_LENGTH}\")\n\n# Show formatted sample\nprint(f\"\\n{'='*60}\")\nprint(\"SAMPLE FORMATTED TRAINING EXAMPLE:\")\nprint(f\"{'='*60}\")\nsample = train_dataset[0]\nfor msg in sample['messages']:\n    print(f\"\\n[{msg['role'].upper()}]\")\n    print(msg['content'])\n\n# Show a few more examples\nprint(f\"\\n{'='*60}\")\nprint(\"MORE EXAMPLES (showing input -> output):\")\nprint(f\"{'='*60}\")\nfor i in range(min(5, len(train_dataset))):\n    sample = train_dataset[i]\n    user_msg = sample['messages'][1]['content']\n    assistant_msg = sample['messages'][2]['content']\n    print(f\"  Q: {user_msg[:60]}...\")\n    print(f\"  A: {assistant_msg}\")\n    print()"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is59DGTAPxGk"
      },
      "source": [
        "## 7. Load Model and Apply LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yg871zcbPxGk",
        "outputId": "ff3e6f08-f46d-469d-afe4-f2f21a3b8656"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer: Qwen/Qwen2.5-0.5B-Instruct\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TOKENIZER DEBUG INFO\n",
            "============================================================\n",
            "  Vocab size: 151643\n",
            "  Pad token: <|endoftext|> (id=151643)\n",
            "  EOS token: <|im_end|> (id=151645)\n",
            "  Padding side: left\n",
            "  Has chat template: True\n",
            "  Chat template test: '<|im_start|>system\n",
            "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
            "<|...'\n",
            "\n",
            "============================================================\n",
            "LOADING MODEL: Qwen/Qwen2.5-0.5B-Instruct\n",
            "============================================================\n",
            "\n",
            "MODEL DEBUG INFO\n",
            "  Total parameters: 494,032,768\n",
            "  Trainable parameters: 494,032,768\n",
            "  Model dtype: torch.bfloat16\n",
            "  Device: cuda:0\n",
            "  Model class: Qwen2ForCausalLM\n",
            "  Hidden size: 896\n",
            "  Num layers: 24\n",
            "  Num heads: 14\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "\n",
        "# Load tokenizer\n",
        "print(f\"Loading tokenizer: {MODEL_NAME}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# IMPORTANT: Use left-padding for decoder-only models during batched generation\n",
        "tokenizer.padding_side = 'left'\n",
        "\n",
        "# Debug: Tokenizer info\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"TOKENIZER DEBUG INFO\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"  Vocab size: {tokenizer.vocab_size}\")\n",
        "print(f\"  Pad token: {tokenizer.pad_token} (id={tokenizer.pad_token_id})\")\n",
        "print(f\"  EOS token: {tokenizer.eos_token} (id={tokenizer.eos_token_id})\")\n",
        "print(f\"  Padding side: {tokenizer.padding_side}\")\n",
        "print(f\"  Has chat template: {tokenizer.chat_template is not None}\")\n",
        "\n",
        "# Test chat template\n",
        "test_msgs = [{\"role\": \"user\", \"content\": \"test\"}]\n",
        "test_prompt = tokenizer.apply_chat_template(test_msgs, tokenize=False, add_generation_prompt=True)\n",
        "print(f\"  Chat template test: '{test_prompt[:100]}...'\")\n",
        "\n",
        "# Load model\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"LOADING MODEL: {MODEL_NAME}\")\n",
        "print(f\"{'='*60}\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# Disable cache and enable gradient checkpointing for training\n",
        "model.config.use_cache = False\n",
        "model.gradient_checkpointing_enable()\n",
        "if hasattr(model, \"enable_input_require_grads\"):\n",
        "    model.enable_input_require_grads()\n",
        "\n",
        "# Debug: Model info\n",
        "print(f\"\\nMODEL DEBUG INFO\")\n",
        "print(f\"  Total parameters: {model.num_parameters():,}\")\n",
        "print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "print(f\"  Model dtype: {model.dtype}\")\n",
        "print(f\"  Device: {model.device}\")\n",
        "print(f\"  Model class: {model.__class__.__name__}\")\n",
        "\n",
        "# Check model architecture\n",
        "if hasattr(model, 'config'):\n",
        "    print(f\"  Hidden size: {model.config.hidden_size}\")\n",
        "    print(f\"  Num layers: {model.config.num_hidden_layers}\")\n",
        "    print(f\"  Num heads: {model.config.num_attention_heads}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4Ea3EXDPxGk",
        "outputId": "a6f6b286-444b-4abf-955c-c293485d370e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "LORA CONFIGURATION DEBUG\n",
            "============================================================\n",
            "  Rank (r): 32\n",
            "  Alpha: 64\n",
            "  Dropout: 0.05\n",
            "  Target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
            "\n",
            "LoRA applied successfully!\n",
            "trainable params: 17,596,416 || all params: 511,629,184 || trainable%: 3.4393\n",
            "\n",
            "LoRA MODULES DEBUG:\n",
            "  Total LoRA modules: 1512\n",
            "  Sample LoRA layers: ['base_model.model.model.layers.0.self_attn.q_proj.lora_dropout', 'base_model.model.model.layers.0.self_attn.q_proj.lora_dropout.default', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B']\n"
          ]
        }
      ],
      "source": [
        "# Configure LoRA\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"LORA CONFIGURATION DEBUG\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"  Rank (r): {LORA_R}\")\n",
        "print(f\"  Alpha: {LORA_ALPHA}\")\n",
        "print(f\"  Dropout: {LORA_DROPOUT}\")\n",
        "print(f\"  Target modules: {LORA_TARGET_MODULES}\")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=LORA_TARGET_MODULES,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "print(f\"\\nLoRA applied successfully!\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Debug: Show which layers have LoRA\n",
        "print(f\"\\nLoRA MODULES DEBUG:\")\n",
        "lora_layers = [name for name, _ in model.named_modules() if 'lora' in name.lower()]\n",
        "print(f\"  Total LoRA modules: {len(lora_layers)}\")\n",
        "print(f\"  Sample LoRA layers: {lora_layers[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QkqsVJJPxGk"
      },
      "source": [
        "## 8. Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668,
          "referenced_widgets": [
            "0330756876c3461b9f0a5cd7af88c60a",
            "6dd2d50ec5fd4419aa7c6e13f0c3dff6",
            "134c76d886374391b0dc89036fcaf293",
            "9a37fdb681f14fbd8d0077f6d2a5c41e",
            "d7504239196842169e0ca519aa15c810",
            "b08cbde14fe047d6a100baa84f1c9b8a",
            "a4b99435b5ff404cabcef1181c07f5fa",
            "5eb7d76481dc43c980f7bcf235545588",
            "04405d9bec7a40b494e5ef1031312f64",
            "ff2c68b960f64320b3f56186ab960b93",
            "fa7b53015a594d5180a431f90d6dbe07",
            "d24ef17b494a44d8b09a0d002a8ce160",
            "eb31167477c3403f816ad56d52b270eb",
            "02ad53e59e474173b82af4cac8ef922b",
            "558f12a7b76a40938991573ff117cd9b",
            "ebbb1c5af8664afbac6a839b29132958",
            "5e2e5029ff974f9a84e3869a7b16c7fd",
            "56e02d0331e14c07a036eaba72b9540d",
            "04499d79018740f4998f5d0cc4bbfdbf",
            "113dbff4101b463e843bd4f5e8e17df1",
            "e65c234e787e4fc3a7e7cce942268ba1",
            "b3582c5fbf1644faa212e30b14ecec97",
            "0937db62a313452e8aa86284305f1e24",
            "14f3cef31151483cba15e135b9267425",
            "314d8c04b4c54c78b556fa10fb8b42ef",
            "be83a74e71e34326b2ceb31ed933075d",
            "b244e35611664ffca2c49282a7b3724a",
            "6517d470e0094ec0b7de0710f4152a7e",
            "889fc77dc2f644038b6c40c368708a12",
            "b32258734e6f4678b69f6f04747d2912",
            "f6089f4cc77c41539a7153656b90ac29",
            "52f49a3a055044e1a3e3f09a43a5da62",
            "6696374d4c374ef0aa80f5ad391b2922",
            "4992da1f3cae461bb2abea6032bffabd",
            "fbacc5739cb64ffb9f3f84a9dbffb954",
            "40347b1313c6493c96ca15e18e57533f",
            "76323bff8f0c493788863bea8d0d413f",
            "9682b4a873ee4594933eb8e86ab52132",
            "851e67ea3369442cac016805fdbccd7e",
            "da73aaa87a2848df94a0f05635d0d987",
            "f4ed8d92dff844ae9a10bfecce11edd2",
            "6917d0f94b684328aa3957c07aad52f6",
            "8e18c44f63344a328e23b7dd61d53814",
            "969be0adf83648729d5fc7632d2cdb47"
          ]
        },
        "id": "GE7tPCJJPxGl",
        "outputId": "641a2815-6545-462d-d40a-59b1c6ccf2a8"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "from transformers import set_seed, TrainerCallback, EarlyStoppingCallback\n",
        "from sklearn.metrics import accuracy_score\n",
        "import re\n",
        "\n",
        "# Set seed\n",
        "set_seed(SEED)\n",
        "\n",
        "# ============================================================\n",
        "# SIMPLIFIED CALLBACK: Direct Intent Accuracy\n",
        "# ============================================================\n",
        "\n",
        "def extract_intent_direct(response, intent_tags):\n",
        "    \"\"\"Extract intent from direct model response (just the tag).\"\"\"\n",
        "    response = response.strip().lower()\n",
        "    \n",
        "    # Direct match - the response should BE the intent tag\n",
        "    for intent in intent_tags:\n",
        "        if intent.lower() == response:\n",
        "            return intent\n",
        "    \n",
        "    # Partial match - if the response contains an intent tag\n",
        "    for intent in intent_tags:\n",
        "        if intent.lower() in response:\n",
        "            return intent\n",
        "    \n",
        "    return None\n",
        "\n",
        "def evaluate_model_direct(model, tokenizer, eval_df, batch_size=32, num_samples=None):\n",
        "    \"\"\"Evaluate model with direct classification format.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    if num_samples:\n",
        "        eval_df = eval_df.sample(n=min(num_samples, len(eval_df)), random_state=42).reset_index(drop=True)\n",
        "\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    raw_outputs = []\n",
        "\n",
        "    num_batches = (len(eval_df) + batch_size - 1) // batch_size\n",
        "    for i in tqdm(range(0, len(eval_df), batch_size), total=num_batches, desc=\"Evaluating\"):\n",
        "        batch_df = eval_df.iloc[i:i+batch_size]\n",
        "\n",
        "        # Create minimal prompts\n",
        "        batch_prompts = []\n",
        "        for q in batch_df['question']:\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": q}\n",
        "            ]\n",
        "            prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "            batch_prompts.append(prompt)\n",
        "\n",
        "        inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True,\n",
        "                          truncation=True, max_length=MAX_SEQ_LENGTH)\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=50,\n",
        "                do_sample=False,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "            )\n",
        "\n",
        "        input_len = inputs['input_ids'].shape[1]\n",
        "        for j, output in enumerate(outputs):\n",
        "            response = tokenizer.decode(output[input_len:], skip_special_tokens=True)\n",
        "            if len(raw_outputs) < 10:\n",
        "                raw_outputs.append(response[:80])\n",
        "            predictions.append(extract_intent_direct(response, INTENT_TAGS))\n",
        "\n",
        "        true_labels.extend(batch_df['tag'].tolist())\n",
        "\n",
        "    # Show sample outputs\n",
        "    print(\"\\nSample outputs:\")\n",
        "    for i, resp in enumerate(raw_outputs[:10]):\n",
        "        true = true_labels[i]\n",
        "        pred = predictions[i]\n",
        "        match = \"✓\" if pred == true else \"✗\"\n",
        "        print(f\"  [{i+1}] {match} '{resp}' | True: {true}\")\n",
        "\n",
        "    return predictions, true_labels\n",
        "\n",
        "def classify_intent(query, model, tokenizer):\n",
        "    \"\"\"Classify intent for a single Bengali query.\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "    intent = extract_intent_direct(response, INTENT_TAGS)\n",
        "\n",
        "    return intent, response\n",
        "\n",
        "class IntentAccuracyCallback(TrainerCallback):\n",
        "    \"\"\"Callback to compute intent accuracy during training.\"\"\"\n",
        "\n",
        "    def __init__(self, eval_df, tokenizer, intent_tags, max_seq_length, \n",
        "                 sample_size=500, batch_size=32, patience=3):\n",
        "        self.eval_sample = eval_df.sample(n=min(sample_size, len(eval_df)), random_state=42).reset_index(drop=True)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.intent_tags = intent_tags\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.batch_size = batch_size\n",
        "        self.patience = patience\n",
        "        self.best_accuracy = 0.0\n",
        "        self.no_improve_count = 0\n",
        "        self.history = []\n",
        "        self.train_losses = []\n",
        "        print(f\"[CALLBACK INIT] Eval samples: {len(self.eval_sample)}, batch_size={batch_size}\")\n",
        "\n",
        "    def on_train_begin(self, args, state, control, **kwargs):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"[DEBUG] TRAINING STARTED\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"  Max steps: {state.max_steps}\")\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs and 'loss' in logs:\n",
        "            self.train_losses.append(logs['loss'])\n",
        "            if state.global_step % 100 == 0:\n",
        "                avg = sum(self.train_losses[-10:]) / min(10, len(self.train_losses))\n",
        "                print(f\"  [Step {state.global_step}] loss={logs['loss']:.4f} | avg_10={avg:.4f}\")\n",
        "\n",
        "    def _compute_accuracy_batched(self, model):\n",
        "        \"\"\"Compute intent accuracy on sample using batched inference.\"\"\"\n",
        "        torch.cuda.empty_cache()\n",
        "        model.eval()\n",
        "        predictions = []\n",
        "        true_labels = self.eval_sample['tag'].tolist()\n",
        "        debug_responses = []\n",
        "\n",
        "        for i in range(0, len(self.eval_sample), self.batch_size):\n",
        "            batch_df = self.eval_sample.iloc[i:i+self.batch_size]\n",
        "\n",
        "            # Create minimal prompts\n",
        "            batch_prompts = []\n",
        "            for q in batch_df['question']:\n",
        "                messages = [\n",
        "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                    {\"role\": \"user\", \"content\": q}\n",
        "                ]\n",
        "                prompt = self.tokenizer.apply_chat_template(\n",
        "                    messages, tokenize=False, add_generation_prompt=True\n",
        "                )\n",
        "                batch_prompts.append(prompt)\n",
        "\n",
        "            inputs = self.tokenizer(batch_prompts, return_tensors=\"pt\", padding=True,\n",
        "                                   truncation=True, max_length=self.max_seq_length)\n",
        "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=50,  # Short - just the tag\n",
        "                    do_sample=False,\n",
        "                    pad_token_id=self.tokenizer.pad_token_id,\n",
        "                )\n",
        "\n",
        "            input_len = inputs['input_ids'].shape[1]\n",
        "            for idx, output in enumerate(outputs):\n",
        "                response = self.tokenizer.decode(output[input_len:], skip_special_tokens=True)\n",
        "                if len(debug_responses) < 5:\n",
        "                    debug_responses.append(response[:100])\n",
        "                predictions.append(extract_intent_direct(response, self.intent_tags))\n",
        "\n",
        "        # Debug output\n",
        "        print(f\"\\n    [DEBUG] Sample outputs (first 5):\")\n",
        "        for i, resp in enumerate(debug_responses[:5]):\n",
        "            true = true_labels[i]\n",
        "            pred = predictions[i]\n",
        "            match = \"✓\" if pred == true else \"✗\"\n",
        "            print(f\"      {match} Raw: '{resp}' | True: {true} | Pred: {pred}\")\n",
        "\n",
        "        # Compute accuracy\n",
        "        valid_pairs = [(p, t) for p, t in zip(predictions, true_labels) if p is not None]\n",
        "        num_none = sum(1 for p in predictions if p is None)\n",
        "        print(f\"    [DEBUG] Valid: {len(valid_pairs)}/{len(predictions)} ({num_none} None)\")\n",
        "\n",
        "        if len(valid_pairs) == 0:\n",
        "            return 0.0\n",
        "        valid_preds, valid_true = zip(*valid_pairs)\n",
        "        return accuracy_score(valid_true, valid_preds)\n",
        "\n",
        "    def on_evaluate(self, args, state, control, model, **kwargs):\n",
        "        accuracy = self._compute_accuracy_batched(model)\n",
        "        self.history.append({'step': state.global_step, 'intent_accuracy': accuracy})\n",
        "\n",
        "        print(f\"\\n>>> Intent Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "        if accuracy > self.best_accuracy + 0.001:\n",
        "            self.best_accuracy = accuracy\n",
        "            self.no_improve_count = 0\n",
        "            print(f\"    [NEW BEST] Best accuracy: {self.best_accuracy:.4f}\")\n",
        "        else:\n",
        "            self.no_improve_count += 1\n",
        "            print(f\"    No improvement for {self.no_improve_count}/{self.patience} evals\")\n",
        "\n",
        "        if self.no_improve_count >= self.patience:\n",
        "            print(f\"\\n*** EARLY STOPPING: No improvement for {self.patience} evals ***\")\n",
        "            control.should_training_stop = True\n",
        "\n",
        "        return control\n",
        "\n",
        "# ============================================================\n",
        "# TRAINING CONFIGURATION - Optimized for L4 24GB\n",
        "# ============================================================\n",
        "\n",
        "training_args = SFTConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "\n",
        "    # Training schedule\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
        "\n",
        "    # Optimizer\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"adamw_8bit\",\n",
        "\n",
        "    # Mixed precision\n",
        "    bf16=True,\n",
        "\n",
        "    # Packing for efficiency\n",
        "    packing=True,\n",
        "    max_length=MAX_SEQ_LENGTH,\n",
        "\n",
        "    # Logging & saving\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    save_total_limit=3,\n",
        "\n",
        "    # Other\n",
        "    seed=SEED,\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        ")\n",
        "\n",
        "# Create callbacks\n",
        "intent_callback = IntentAccuracyCallback(\n",
        "    eval_df=eval_df,\n",
        "    tokenizer=tokenizer,\n",
        "    intent_tags=INTENT_TAGS,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    sample_size=500,\n",
        "    batch_size=EVAL_BATCH_SIZE,\n",
        "    patience=EARLY_STOPPING_PATIENCE,\n",
        ")\n",
        "\n",
        "early_stopping_callback = EarlyStoppingCallback(\n",
        "    early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
        "    early_stopping_threshold=0.001,\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    args=training_args,\n",
        "    callbacks=[early_stopping_callback, intent_callback],\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# DEBUG: Training setup info\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"TRAINING SETUP DEBUG INFO\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Max sequence length: {MAX_SEQ_LENGTH}\")\n",
        "print(f\"  Packing: ENABLED\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Gradient accumulation: {GRAD_ACCUM_STEPS}\")\n",
        "print(f\"  Effective batch size: {BATCH_SIZE * GRAD_ACCUM_STEPS}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  Eval every: {training_args.eval_steps} steps\")\n",
        "print(f\"  Early stopping patience: {EARLY_STOPPING_PATIENCE}\")\n",
        "\n",
        "# Estimate VRAM\n",
        "print(f\"\\n  Estimated VRAM usage: ~12-16GB (L4 has 24GB)\")\n",
        "\n",
        "# Quick generation test\n",
        "print(f\"\\nQUICK GENERATION TEST (before training):\")\n",
        "test_messages = [\n",
        "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "    {\"role\": \"user\", \"content\": \"একাউন্ট লক হয়ে গেছে\"}\n",
        "]\n",
        "test_prompt = tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)\n",
        "test_inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
        "with torch.no_grad():\n",
        "    test_out = model.generate(**test_inputs, max_new_tokens=30, do_sample=False, pad_token_id=tokenizer.pad_token_id)\n",
        "test_response = tokenizer.decode(test_out[0][test_inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "print(f\"  Query: একাউন্ট লক হয়ে গেছে\")\n",
        "print(f\"  Output: '{test_response}'\")\n",
        "print(f\"\\nTrainer ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "JofKkDKvPxGl",
        "outputId": "d3710bbb-a79f-4470-9bf0-ebb2ecc59c56"
      },
      "outputs": [],
      "source": [
        "# Train!\n",
        "print(\"Starting training...\")\n",
        "print(f\"Dataset: ~78k Bengali NID queries, 407 intents\")\n",
        "print(f\"Format: DIRECT CLASSIFICATION (no CoT, no few-shot)\")\n",
        "print(f\"Epochs: {NUM_EPOCHS} (with early stopping)\")\n",
        "print(f\"Packing: ENABLED for efficiency\")\n",
        "print(\"\")\n",
        "print(\"Expected behavior:\")\n",
        "print(\"  - Model outputs just the intent tag (e.g., 'account_locked')\")\n",
        "print(\"  - No reasoning, no examples, just direct classification\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "trainer.train(resume_from_checkpoint=RESUME_FROM_CHECKPOINT)\n",
        "\n",
        "# ============================================================\n",
        "# POST-TRAINING DEBUG INFO\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"TRAINING COMPLETE - DEBUG SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"  Best intent accuracy: {intent_callback.best_accuracy:.4f} ({intent_callback.best_accuracy*100:.2f}%)\")\n",
        "print(f\"  Total evaluations: {len(intent_callback.history)}\")\n",
        "\n",
        "# Show accuracy progression\n",
        "print(f\"\\nACCURACY PROGRESSION:\")\n",
        "for h in intent_callback.history:\n",
        "    print(f\"  Step {h['step']}: {h['intent_accuracy']:.4f} ({h['intent_accuracy']*100:.1f}%)\")\n",
        "\n",
        "# Post-training generation test\n",
        "print(f\"\\nPOST-TRAINING GENERATION TEST:\")\n",
        "test_queries = [\n",
        "    \"একাউন্ট লক হয়ে গেছে কি করব?\",\n",
        "    \"কার্ড হারিয়ে গেছে\",\n",
        "    \"নাম সংশোধন করতে চাই\"\n",
        "]\n",
        "for q in test_queries:\n",
        "    test_messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": q}\n",
        "    ]\n",
        "    test_prompt = tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)\n",
        "    test_inputs = tokenizer(test_prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        test_out = model.generate(**test_inputs, max_new_tokens=50, do_sample=False, pad_token_id=tokenizer.pad_token_id)\n",
        "    test_response = tokenizer.decode(test_out[0][test_inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "    print(f\"  Q: {q}\")\n",
        "    print(f\"  A: {test_response}\")\n",
        "# Save SFT model + mappings before DPO\n",
        "print(f\"\\nSaving SFT model to {OUTPUT_DIR}...\")\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "import json\n",
        "with open(f\"{OUTPUT_DIR}/intent_mappings.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\"id2intent\": ID2INTENT, \"intent2id\": INTENT2ID}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "with open(f\"{OUTPUT_DIR}/training_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\n",
        "        \"stage\": \"sft\",\n",
        "        \"base_model\": MODEL_NAME,\n",
        "        \"run_name\": RUN_NAME,\n",
        "        \"output_dir\": OUTPUT_DIR,\n",
        "        \"dpo_output_dir\": DPO_OUTPUT_DIR,\n",
        "    }, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"SFT model saved successfully!\")\n",
        "print(f\"Files in {OUTPUT_DIR}:\")\n",
        "!ls -la {OUTPUT_DIR}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": "## 8.1 DPO Training (Preference Optimization)\n\nAfter SFT, we apply DPO to improve accuracy on hard/ambiguous intents.\nThis trains the model to prefer correct intents over wrong predictions.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# ============================================================\n# STEP 1: Build DPO Preference Dataset from Hard Cases\n# ============================================================\n\nfrom collections import defaultdict\nfrom tqdm import tqdm\n\ndef create_dpo_dataset_from_errors(model, tokenizer, eval_df, intent_tags,\n                                    batch_size=32, num_samples=5000):\n    \"\"\"\n    Run inference on eval set, collect wrong predictions as preference pairs.\n    Format: prompt -> (chosen=correct_tag, rejected=wrong_prediction)\n    \"\"\"\n    model.eval()\n    preference_pairs = []\n    \n    sample_df = eval_df.sample(n=min(num_samples, len(eval_df)), random_state=42).reset_index(drop=True)\n    \n    print(f\"Collecting model errors from {len(sample_df)} samples...\")\n    for i in tqdm(range(0, len(sample_df), batch_size), desc=\"Collecting errors\"):\n        batch_df = sample_df.iloc[i:i+batch_size]\n        \n        # Create prompts\n        batch_prompts = []\n        for q in batch_df['question']:\n            messages = [\n                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                {\"role\": \"user\", \"content\": q}\n            ]\n            prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n            batch_prompts.append(prompt)\n        \n        # Generate predictions\n        inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True,\n                          truncation=True, max_length=MAX_SEQ_LENGTH)\n        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n        \n        with torch.no_grad():\n            outputs = model.generate(**inputs, max_new_tokens=50, do_sample=False,\n                                    pad_token_id=tokenizer.pad_token_id)\n        \n        input_len = inputs['input_ids'].shape[1]\n        for idx, (output, (_, row)) in enumerate(zip(outputs, batch_df.iterrows())):\n            response = tokenizer.decode(output[input_len:], skip_special_tokens=True).strip()\n            true_tag = row['tag']\n            pred_tag = extract_intent_direct(response, intent_tags)\n            \n            # Only collect WRONG predictions as preference pairs\n            if pred_tag is not None and pred_tag != true_tag:\n                preference_pairs.append({\n                    \"prompt\": batch_prompts[idx],\n                    \"chosen\": true_tag,      # Correct intent\n                    \"rejected\": pred_tag,    # Model's wrong prediction\n                })\n    \n    return preference_pairs\n\ndef add_synthetic_negatives(train_df, tokenizer, num_samples=3000):\n    \"\"\"Create pairs with similar-looking tags as negatives.\"\"\"\n    pairs = []\n    tags_by_prefix = defaultdict(list)\n    \n    # Group tags by prefix (e.g., address_change_*)\n    for tag in train_df['tag'].unique():\n        prefix = tag.split('_')[0]\n        tags_by_prefix[prefix].append(tag)\n    \n    print(f\"Creating synthetic hard negatives from {num_samples} samples...\")\n    sample_df = train_df.sample(n=min(num_samples, len(train_df)), random_state=42)\n    \n    for _, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Synthetic pairs\"):\n        true_tag = row['tag']\n        prefix = true_tag.split('_')[0]\n        \n        # Pick a similar tag (same prefix) as hard negative\n        similar_tags = [t for t in tags_by_prefix[prefix] if t != true_tag]\n        if similar_tags:\n            wrong_tag = random.choice(similar_tags)\n            \n            messages = [\n                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                {\"role\": \"user\", \"content\": row['question']}\n            ]\n            prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n            \n            pairs.append({\n                \"prompt\": prompt,\n                \"chosen\": true_tag,\n                \"rejected\": wrong_tag,\n            })\n    \n    return pairs\n\n# Collect error-based preference pairs\nprint(f\"\\n{'='*60}\")\nprint(\"BUILDING DPO PREFERENCE DATASET\")\nprint(f\"{'='*60}\")\n\nerror_pairs = create_dpo_dataset_from_errors(\n    model, tokenizer, eval_df, INTENT_TAGS, \n    batch_size=EVAL_BATCH_SIZE, num_samples=5000\n)\nprint(f\"  Error pairs collected: {len(error_pairs)}\")\n\n# Add synthetic hard negatives\nsynthetic_pairs = add_synthetic_negatives(train_df, tokenizer, num_samples=3000)\nprint(f\"  Synthetic pairs created: {len(synthetic_pairs)}\")\n\n# Combine all pairs\nall_pairs = error_pairs + synthetic_pairs\nrandom.shuffle(all_pairs)\nprint(f\"  Total DPO pairs: {len(all_pairs)}\")\n\n# Show sample pairs\nprint(f\"\\nSample preference pairs:\")\nfor i, pair in enumerate(all_pairs[:3]):\n    print(f\"  [{i+1}] Chosen: {pair['chosen']} | Rejected: {pair['rejected']}\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 2: DPO Training\n",
        "# ============================================================\n",
        "\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "from datasets import Dataset\n",
        "\n",
        "# Create HuggingFace Dataset from preference pairs\n",
        "dpo_dataset = Dataset.from_list(all_pairs)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"DPO TRAINING CONFIGURATION\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# DPO Config - optimized for L4 24GB\n",
        "dpo_config = DPOConfig(\n",
        "    output_dir=DPO_OUTPUT_DIR,\n",
        "    \n",
        "    # Training schedule\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=4,\n",
        "    \n",
        "    # DPO-specific\n",
        "    beta=0.1,  # Preference strength (0.05-0.2 range)\n",
        "    loss_type=\"sigmoid\",  # Best for classification\n",
        "    \n",
        "    # Sequence lengths\n",
        "    max_prompt_length=200,\n",
        "    max_length=256,\n",
        "    \n",
        "    # Optimizer\n",
        "    learning_rate=5e-7,  # Low LR for preference fine-tuning\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"adamw_8bit\",\n",
        "    \n",
        "    # Memory\n",
        "    bf16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    \n",
        "    # Logging\n",
        "    logging_steps=20,\n",
        "    eval_strategy=\"no\",  # Skip eval during DPO\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    \n",
        "    seed=SEED,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "print(f\"  Preference pairs: {len(dpo_dataset)}\")\n",
        "print(f\"  Beta: {dpo_config.beta}\")\n",
        "print(f\"  Learning rate: {dpo_config.learning_rate}\")\n",
        "print(f\"  Batch size: {dpo_config.per_device_train_batch_size} x {dpo_config.gradient_accumulation_steps}\")\n",
        "print(f\"  Epochs: {dpo_config.num_train_epochs}\")\n",
        "\n",
        "# Initialize DPO Trainer\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    ref_model=None,  # Auto-creates frozen copy\n",
        "    args=dpo_config,\n",
        "    train_dataset=dpo_dataset,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"STARTING DPO TRAINING\")\n",
        "print(f\"{'='*60}\")\n",
        "print(\"This will teach the model to prefer correct intents over wrong ones...\")\n",
        "\n",
        "dpo_trainer.train(resume_from_checkpoint=DPO_RESUME_FROM_CHECKPOINT)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"DPO TRAINING COMPLETE\")\n",
        "print(f\"{'='*60}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# ============================================================\n# STEP 3: Evaluate DPO-improved Model\n# ============================================================\n\nprint(f\"\\n{'='*60}\")\nprint(\"POST-DPO EVALUATION\")\nprint(f\"{'='*60}\")\n\n# Re-run evaluation on the same samples\npredictions_dpo, true_labels_dpo = evaluate_model_direct(\n    model, tokenizer, eval_df, batch_size=EVAL_BATCH_SIZE, num_samples=2000\n)\n\n# Calculate metrics\nvalid_mask_dpo = [p is not None for p in predictions_dpo]\nvalid_preds_dpo = [INTENT2ID.get(p, -1) for p in predictions_dpo]\nvalid_true_dpo = [INTENT2ID.get(t, -1) for t in true_labels_dpo]\n\nfiltered_preds_dpo = [p for p, m in zip(valid_preds_dpo, valid_mask_dpo) if m and p != -1]\nfiltered_true_dpo = [t for t, m, p in zip(valid_true_dpo, valid_mask_dpo, valid_preds_dpo) if m and p != -1]\n\nif len(filtered_preds_dpo) > 0:\n    accuracy_dpo = accuracy_score(filtered_true_dpo, filtered_preds_dpo)\nelse:\n    accuracy_dpo = 0.0\n\nnull_preds_dpo = sum(1 for p in predictions_dpo if p is None)\n\n# Compare results\nprint(f\"\\n{'='*60}\")\nprint(\"RESULTS COMPARISON: Before vs After DPO\")\nprint(f\"{'='*60}\")\nprint(f\"  Before DPO: ~70.0% accuracy, ~0.4% None predictions\")\nprint(f\"  After DPO:  {accuracy_dpo*100:.1f}% accuracy, {100*null_preds_dpo/len(predictions_dpo):.1f}% None\")\nprint(f\"\")\nif accuracy_dpo > 0.70:\n    improvement = (accuracy_dpo - 0.70) * 100\n    print(f\"  Improvement: +{improvement:.1f}%\")\nelse:\n    print(f\"  Note: Run more epochs or adjust beta if accuracy didn't improve\")\n\n# Show sample outputs after DPO\nprint(f\"\\nPOST-DPO TEST QUERIES:\")\ntest_queries_dpo = [\n    \"একাউন্ট লক হয়ে গেছে কি করব?\",\n    \"কার্ড হারিয়ে গেছে\",\n    \"নাম সংশোধন করতে চাই\",\n    \"ঠিকানা পরিবর্তন করতে চাই\",\n]\nfor q in test_queries_dpo:\n    intent, raw = classify_intent(q, model, tokenizer)\n    print(f\"  Q: {q}\")\n    print(f\"  A: {raw} -> {intent}\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx_nMO3lPxGl"
      },
      "source": [
        "## 9. Save DPO Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUJptZpfPxGl"
      },
      "outputs": [],
      "source": [
        "# Save DPO model to Google Drive\n",
        "if 'dpo_trainer' in globals():\n",
        "    print(f\"Saving DPO model to {DPO_OUTPUT_DIR}...\")\n",
        "    dpo_trainer.save_model(DPO_OUTPUT_DIR)\n",
        "    tokenizer.save_pretrained(DPO_OUTPUT_DIR)\n",
        "\n",
        "    # Also save intent mappings + metadata\n",
        "    import json\n",
        "    with open(f\"{DPO_OUTPUT_DIR}/intent_mappings.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\"id2intent\": ID2INTENT, \"intent2id\": INTENT2ID}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    with open(f\"{DPO_OUTPUT_DIR}/training_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\n",
        "            \"stage\": \"dpo\",\n",
        "            \"base_model\": MODEL_NAME,\n",
        "            \"run_name\": RUN_NAME,\n",
        "            \"output_dir\": OUTPUT_DIR,\n",
        "            \"dpo_output_dir\": DPO_OUTPUT_DIR,\n",
        "        }, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(\"DPO model saved successfully!\")\n",
        "    print(f\"Files in {DPO_OUTPUT_DIR}:\")\n",
        "    !ls -la {DPO_OUTPUT_DIR}\n",
        "else:\n",
        "    print(\"DPO trainer not found. Skipping DPO save.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5nM-792PxGl"
      },
      "source": [
        "## 10. Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fw_Vo7pQPxGl"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from collections import Counter\n",
        "\n",
        "# Evaluate\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"EVALUATION\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"  Batch size: {EVAL_BATCH_SIZE}\")\n",
        "print(f\"  Num samples: 2000\")\n",
        "print(f\"\\nStarting evaluation...\")\n",
        "\n",
        "predictions, true_labels = evaluate_model_direct(\n",
        "    model, tokenizer, eval_df, batch_size=EVAL_BATCH_SIZE, num_samples=2000\n",
        ")\n",
        "\n",
        "# Debug: Prediction distribution\n",
        "print(f\"\\nPREDICTION DEBUG:\")\n",
        "null_preds = sum(1 for p in predictions if p is None)\n",
        "print(f\"  Null predictions: {null_preds}/{len(predictions)} ({100*null_preds/len(predictions):.1f}%)\")\n",
        "pred_counts = Counter([p for p in predictions if p])\n",
        "top_preds = pred_counts.most_common(10)\n",
        "print(f\"  Top 10 predicted tags: {top_preds}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95mlnxeQPxGl"
      },
      "outputs": [],
      "source": "# Compute metrics\nvalid_mask = [p is not None for p in predictions]\nvalid_preds = [INTENT2ID.get(p, -1) for p in predictions]\nvalid_true = [INTENT2ID.get(t, -1) for t in true_labels]\n\n# Filter valid\nfiltered_preds = [p for p, m in zip(valid_preds, valid_mask) if m and p != -1]\nfiltered_true = [t for t, m, p in zip(valid_true, valid_mask, valid_preds) if m and p != -1]\n\n# Calculate metrics\nif len(filtered_preds) > 0:\n    accuracy = accuracy_score(filtered_true, filtered_preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        filtered_true, filtered_preds, average=\"weighted\", zero_division=0\n    )\nelse:\n    accuracy = precision = recall = f1 = 0.0\n\nprint(\"=\" * 50)\nprint(\"EVALUATION RESULTS\")\nprint(\"=\" * 50)\nprint(f\"Total samples: {len(predictions)}\")\nprint(f\"Valid predictions: {sum(valid_mask)} ({100*sum(valid_mask)/len(predictions):.1f}%)\")\nprint(f\"\")\nprint(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall:    {recall:.4f}\")\nprint(f\"F1 Score:  {f1:.4f}\")\nprint(\"=\" * 50)\n\n# Show top confusions\nprint(\"\\nTop 10 Confusions (True -> Predicted):\")\nconfusions = [(t, p) for p, t in zip(predictions, true_labels) if p != t and p is not None]\nfor (true, pred), count in Counter(confusions).most_common(10):\n    print(f\"  {true} -> {pred}: {count}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo2IuxB3PxGl"
      },
      "source": [
        "## 11. Interactive Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygtSMSt4PxGl"
      },
      "outputs": [],
      "source": [
        "# Get answer for intent from tag_answer_df\n",
        "def get_answer(intent):\n",
        "    \"\"\"Get Bengali answer for an intent.\"\"\"\n",
        "    row = tag_answer_df[tag_answer_df['tag'] == intent]\n",
        "    if len(row) > 0:\n",
        "        return row.iloc[0]['answer']\n",
        "    return \"উত্তর পাওয়া যায়নি।\"\n",
        "\n",
        "# Test with sample Bengali queries\n",
        "test_queries = [\n",
        "    \"আমার এনআইডি একাউন্ট লক হয়ে গেছে, কিভাবে আনলক করবো?\",\n",
        "    \"কার্ড হারিয়ে গেলে কি করতে হবে?\",\n",
        "    \"জাতীয় পরিচয়পত্রে নাম সংশোধন করতে চাই\",\n",
        "    \"ভোটার আইডি কার্ডের ঠিকানা পরিবর্তন করতে কি কি লাগবে?\",\n",
        "    \"স্মার্ট কার্ড কবে পাবো?\",\n",
        "    \"আমার জন্ম তারিখ ভুল আছে\",\n",
        "    \"NID নম্বর ভুলে গেছি\",\n",
        "]\n",
        "\n",
        "print(\"Testing with Bengali queries (Direct Classification):\")\n",
        "print(\"=\" * 70)\n",
        "for query in test_queries:\n",
        "    intent, raw_output = classify_intent(query, model, tokenizer)\n",
        "    answer = get_answer(intent) if intent else \"Intent not recognized\"\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Raw output: '{raw_output}'\")\n",
        "    print(f\"Intent: {intent}\")\n",
        "    print(f\"Answer: {answer[:100]}...\" if len(answer) > 100 else f\"Answer: {answer}\")\n",
        "    print(\"-\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPQ1tXTnPxGl"
      },
      "source": [
        "## 12. Push to HuggingFace Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azBWZepHPxGl"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "if not HF_TOKEN:\n",
        "    raise ValueError(\"Set HF_TOKEN in the top cell before login.\")\n",
        "\n",
        "login(token=HF_TOKEN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFcKckLiPxGl"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "if not HF_USERNAME:\n",
        "    raise ValueError(\"Set HF_USERNAME in the top cell.\")\n",
        "\n",
        "if not HF_REPO_SUFFIX:\n",
        "    HF_REPO_SUFFIX = f\"{RUN_NAME}-{MODEL_NAME.split('/')[-1].lower()}\"\n",
        "\n",
        "if not HF_RUN_TAG:\n",
        "    HF_RUN_TAG = datetime.utcnow().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "HF_REPO_NAME = f\"{HF_USERNAME}/{HF_REPO_SUFFIX}-{HF_RUN_TAG}\"\n",
        "\n",
        "print(f\"Pushing model to HuggingFace Hub: {HF_REPO_NAME}\")\n",
        "\n",
        "model.push_to_hub(HF_REPO_NAME)\n",
        "tokenizer.push_to_hub(HF_REPO_NAME)\n",
        "\n",
        "print(f\"\\nModel uploaded successfully!\")\n",
        "print(f\"View at: https://huggingface.co/{HF_REPO_NAME}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1YJ41MtPxGl"
      },
      "source": [
        "## Done!\n",
        "\n",
        "Your Bengali NID intent classification model has been trained with:\n",
        "\n",
        "### Training Pipeline:\n",
        "1. **SFT (Supervised Fine-Tuning)** - Direct classification format (~70% accuracy)\n",
        "2. **DPO (Direct Preference Optimization)** - Improves hard/ambiguous intents (+5-10%)\n",
        "\n",
        "### Key Features:\n",
        "- **Direct Classification** - No CoT reasoning, no few-shot examples\n",
        "- **Minimal Format** - System prompt + query → tag only\n",
        "- **Packing Enabled** - More efficient GPU utilization\n",
        "- **Optimized for L4** - ~16GB VRAM usage\n",
        "- **DPO on errors** - Trains on actual model mistakes + hard negatives\n",
        "\n",
        "### Model Locations:\n",
        "- SFT output: `/content/drive/MyDrive/models/smollm2-bengali-nid-intent`\n",
        "- DPO output: `/content/drive/MyDrive/models/smollm2-bengali-nid-intent-dpo`\n",
        "- HuggingFace Hub: `HF_REPO_NAME` printed in the upload cell\n",
        "\n",
        "### To load the model later (HF or local checkpoint):\n",
        "```python\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import glob\n",
        "\n",
        "# Option A: HuggingFace Hub (use the printed HF_REPO_NAME)\n",
        "HF_REPO_NAME = \"your-username/bn-nid-intent-qwen2.5-0.5b-YYYYMMDD-HHMMSS\"\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
        "model = PeftModel.from_pretrained(base_model, HF_REPO_NAME)\n",
        "tokenizer = AutoTokenizer.from_pretrained(HF_REPO_NAME)\n",
        "\n",
        "# Option B: Local checkpoint\n",
        "# ckpts = sorted(glob.glob(\"/content/drive/MyDrive/models/smollm2-bengali-nid-intent/checkpoint-*\"))\n",
        "# checkpoint = ckpts[-1] if ckpts else \"/content/drive/MyDrive/models/smollm2-bengali-nid-intent\"\n",
        "# model = PeftModel.from_pretrained(base_model, checkpoint)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/models/smollm2-bengali-nid-intent\")\n",
        "\n",
        "# Inference\n",
        "SYSTEM_PROMPT = \"You are an intent classifier for Bengali NID customer service. Output only the intent tag, nothing else.\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "    {\"role\": \"user\", \"content\": \"একাউন্ট লক হয়ে গেছে\"}\n",
        "]\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=50, do_sample=False)\n",
        "intent = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "print(intent)  # Should output: account_locked\n",
        "```\n",
        "\n",
        "### Expected Results:\n",
        "| Stage | Accuracy | None % |\n",
        "|-------|----------|--------|\n",
        "| After SFT | ~70% | ~0.4% |\n",
        "| After DPO | ~75-80% | ~0% |"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0330756876c3461b9f0a5cd7af88c60a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6dd2d50ec5fd4419aa7c6e13f0c3dff6",
              "IPY_MODEL_134c76d886374391b0dc89036fcaf293",
              "IPY_MODEL_9a37fdb681f14fbd8d0077f6d2a5c41e"
            ],
            "layout": "IPY_MODEL_d7504239196842169e0ca519aa15c810"
          }
        },
        "6dd2d50ec5fd4419aa7c6e13f0c3dff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b08cbde14fe047d6a100baa84f1c9b8a",
            "placeholder": "​",
            "style": "IPY_MODEL_a4b99435b5ff404cabcef1181c07f5fa",
            "value": "Tokenizing train dataset: 100%"
          }
        },
        "134c76d886374391b0dc89036fcaf293": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5eb7d76481dc43c980f7bcf235545588",
            "max": 78616,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_04405d9bec7a40b494e5ef1031312f64",
            "value": 78616
          }
        },
        "9a37fdb681f14fbd8d0077f6d2a5c41e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff2c68b960f64320b3f56186ab960b93",
            "placeholder": "​",
            "style": "IPY_MODEL_fa7b53015a594d5180a431f90d6dbe07",
            "value": " 78616/78616 [05:46&lt;00:00, 270.84 examples/s]"
          }
        },
        "d7504239196842169e0ca519aa15c810": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b08cbde14fe047d6a100baa84f1c9b8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4b99435b5ff404cabcef1181c07f5fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5eb7d76481dc43c980f7bcf235545588": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04405d9bec7a40b494e5ef1031312f64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff2c68b960f64320b3f56186ab960b93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa7b53015a594d5180a431f90d6dbe07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d24ef17b494a44d8b09a0d002a8ce160": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb31167477c3403f816ad56d52b270eb",
              "IPY_MODEL_02ad53e59e474173b82af4cac8ef922b",
              "IPY_MODEL_558f12a7b76a40938991573ff117cd9b"
            ],
            "layout": "IPY_MODEL_ebbb1c5af8664afbac6a839b29132958"
          }
        },
        "eb31167477c3403f816ad56d52b270eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e2e5029ff974f9a84e3869a7b16c7fd",
            "placeholder": "​",
            "style": "IPY_MODEL_56e02d0331e14c07a036eaba72b9540d",
            "value": "Truncating train dataset: 100%"
          }
        },
        "02ad53e59e474173b82af4cac8ef922b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04499d79018740f4998f5d0cc4bbfdbf",
            "max": 78616,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_113dbff4101b463e843bd4f5e8e17df1",
            "value": 78616
          }
        },
        "558f12a7b76a40938991573ff117cd9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e65c234e787e4fc3a7e7cce942268ba1",
            "placeholder": "​",
            "style": "IPY_MODEL_b3582c5fbf1644faa212e30b14ecec97",
            "value": " 78616/78616 [00:01&lt;00:00, 60954.49 examples/s]"
          }
        },
        "ebbb1c5af8664afbac6a839b29132958": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e2e5029ff974f9a84e3869a7b16c7fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56e02d0331e14c07a036eaba72b9540d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04499d79018740f4998f5d0cc4bbfdbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "113dbff4101b463e843bd4f5e8e17df1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e65c234e787e4fc3a7e7cce942268ba1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3582c5fbf1644faa212e30b14ecec97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0937db62a313452e8aa86284305f1e24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14f3cef31151483cba15e135b9267425",
              "IPY_MODEL_314d8c04b4c54c78b556fa10fb8b42ef",
              "IPY_MODEL_be83a74e71e34326b2ceb31ed933075d"
            ],
            "layout": "IPY_MODEL_b244e35611664ffca2c49282a7b3724a"
          }
        },
        "14f3cef31151483cba15e135b9267425": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6517d470e0094ec0b7de0710f4152a7e",
            "placeholder": "​",
            "style": "IPY_MODEL_889fc77dc2f644038b6c40c368708a12",
            "value": "Tokenizing eval dataset: 100%"
          }
        },
        "314d8c04b4c54c78b556fa10fb8b42ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b32258734e6f4678b69f6f04747d2912",
            "max": 11457,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6089f4cc77c41539a7153656b90ac29",
            "value": 11457
          }
        },
        "be83a74e71e34326b2ceb31ed933075d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52f49a3a055044e1a3e3f09a43a5da62",
            "placeholder": "​",
            "style": "IPY_MODEL_6696374d4c374ef0aa80f5ad391b2922",
            "value": " 11457/11457 [00:51&lt;00:00, 248.75 examples/s]"
          }
        },
        "b244e35611664ffca2c49282a7b3724a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6517d470e0094ec0b7de0710f4152a7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "889fc77dc2f644038b6c40c368708a12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b32258734e6f4678b69f6f04747d2912": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6089f4cc77c41539a7153656b90ac29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "52f49a3a055044e1a3e3f09a43a5da62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6696374d4c374ef0aa80f5ad391b2922": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4992da1f3cae461bb2abea6032bffabd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fbacc5739cb64ffb9f3f84a9dbffb954",
              "IPY_MODEL_40347b1313c6493c96ca15e18e57533f",
              "IPY_MODEL_76323bff8f0c493788863bea8d0d413f"
            ],
            "layout": "IPY_MODEL_9682b4a873ee4594933eb8e86ab52132"
          }
        },
        "fbacc5739cb64ffb9f3f84a9dbffb954": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_851e67ea3369442cac016805fdbccd7e",
            "placeholder": "​",
            "style": "IPY_MODEL_da73aaa87a2848df94a0f05635d0d987",
            "value": "Truncating eval dataset: 100%"
          }
        },
        "40347b1313c6493c96ca15e18e57533f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4ed8d92dff844ae9a10bfecce11edd2",
            "max": 11457,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6917d0f94b684328aa3957c07aad52f6",
            "value": 11457
          }
        },
        "76323bff8f0c493788863bea8d0d413f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e18c44f63344a328e23b7dd61d53814",
            "placeholder": "​",
            "style": "IPY_MODEL_969be0adf83648729d5fc7632d2cdb47",
            "value": " 11457/11457 [00:00&lt;00:00, 42299.50 examples/s]"
          }
        },
        "9682b4a873ee4594933eb8e86ab52132": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "851e67ea3369442cac016805fdbccd7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da73aaa87a2848df94a0f05635d0d987": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4ed8d92dff844ae9a10bfecce11edd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6917d0f94b684328aa3957c07aad52f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e18c44f63344a328e23b7dd61d53814": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "969be0adf83648729d5fc7632d2cdb47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}